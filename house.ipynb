{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE DE CARGA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train = pd.read_csv('Housing Dreams/house_train_raw.csv')\n",
    "house_test = pd.read_csv('Housing Dreams/houses_test_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE DE ANALISIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LotFrontage': [259, 'float64'],\n",
       " 'Alley': [1369, 'object'],\n",
       " 'MasVnrType': [8, 'object'],\n",
       " 'MasVnrArea': [8, 'float64'],\n",
       " 'BsmtQual': [37, 'object'],\n",
       " 'BsmtCond': [37, 'object'],\n",
       " 'BsmtExposure': [38, 'object'],\n",
       " 'BsmtFinType1': [37, 'object'],\n",
       " 'BsmtFinType2': [38, 'object'],\n",
       " 'Electrical': [1, 'object'],\n",
       " 'FireplaceQu': [690, 'object'],\n",
       " 'GarageType': [81, 'object'],\n",
       " 'GarageYrBlt': [81, 'float64'],\n",
       " 'GarageFinish': [81, 'object'],\n",
       " 'GarageQual': [81, 'object'],\n",
       " 'GarageCond': [81, 'object'],\n",
       " 'PoolQC': [1453, 'object'],\n",
       " 'Fence': [1179, 'object'],\n",
       " 'MiscFeature': [1406, 'object']}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns = house_train.columns[house_train.isna().any()].tolist()\n",
    "null_columns_type = {null_columns[i] : [house_train[null_columns[i]].isnull().sum(), str(house_train[null_columns[i]].dtype)] for i in range(len(null_columns))}\n",
    "null_columns_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSZoning': [4, 'object'],\n",
       " 'LotFrontage': [227, 'float64'],\n",
       " 'Alley': [1352, 'object'],\n",
       " 'Utilities': [2, 'object'],\n",
       " 'Exterior1st': [1, 'object'],\n",
       " 'Exterior2nd': [1, 'object'],\n",
       " 'MasVnrType': [16, 'object'],\n",
       " 'MasVnrArea': [15, 'float64'],\n",
       " 'BsmtQual': [44, 'object'],\n",
       " 'BsmtCond': [45, 'object'],\n",
       " 'BsmtExposure': [44, 'object'],\n",
       " 'BsmtFinType1': [42, 'object'],\n",
       " 'BsmtFinSF1': [1, 'float64'],\n",
       " 'BsmtFinType2': [42, 'object'],\n",
       " 'BsmtFinSF2': [1, 'float64'],\n",
       " 'BsmtUnfSF': [1, 'float64'],\n",
       " 'TotalBsmtSF': [1, 'float64'],\n",
       " 'BsmtFullBath': [2, 'float64'],\n",
       " 'BsmtHalfBath': [2, 'float64'],\n",
       " 'KitchenQual': [1, 'object'],\n",
       " 'Functional': [2, 'object'],\n",
       " 'FireplaceQu': [730, 'object'],\n",
       " 'GarageType': [76, 'object'],\n",
       " 'GarageYrBlt': [78, 'float64'],\n",
       " 'GarageFinish': [78, 'object'],\n",
       " 'GarageCars': [1, 'float64'],\n",
       " 'GarageArea': [1, 'float64'],\n",
       " 'GarageQual': [78, 'object'],\n",
       " 'GarageCond': [78, 'object'],\n",
       " 'PoolQC': [1456, 'object'],\n",
       " 'Fence': [1169, 'object'],\n",
       " 'MiscFeature': [1408, 'object'],\n",
       " 'SaleType': [1, 'object']}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns_test = house_test.columns[house_test.isna().any()].tolist()\n",
    "null_columns_type_test = {null_columns_test[i] : [house_test[null_columns_test[i]].isnull().sum(), str(house_test[null_columns_test[i]].dtype)] for i in range(len(null_columns_test))}\n",
    "null_columns_type_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Primer diccionario:** Columnas con valores nulos y su cantidad, para *house_train*\n",
    "* **Segundo diccionario:** Columnas con valores nulos y su cantidad, para *house_test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE DE TRANSFORMACIÓN, PREVIA AL MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train.LotFrontage = house_train.LotFrontage.fillna(house_train.LotFrontage.mean())\n",
    "house_train.MasVnrType = house_train.MasVnrType.fillna(house_train.MasVnrType.mode()[0])\n",
    "house_train.MasVnrArea = house_train.MasVnrArea.fillna(house_train.MasVnrArea.mean())\n",
    "house_train.BsmtQual = house_train.BsmtQual.fillna(house_train.BsmtQual.mode()[0])\n",
    "house_train.BsmtCond = house_train.BsmtCond.fillna(house_train.BsmtCond.mode()[0])\n",
    "house_train.BsmtExposure = house_train.BsmtExposure.fillna(house_train.BsmtExposure.mode()[0])\n",
    "house_train.BsmtFinType1 = house_train.BsmtFinType1.fillna(house_train.BsmtFinType1.mode()[0])\n",
    "house_train.BsmtFinType2 = house_train.BsmtFinType2.fillna(house_train.BsmtFinType2.mode()[0])\n",
    "house_train.Electrical = house_train.Electrical.fillna(house_train.Electrical.mode()[0])\n",
    "house_train.FireplaceQu = house_train.FireplaceQu.fillna(house_train.FireplaceQu.mode()[0])\n",
    "house_train.GarageType = house_train.GarageType.fillna(house_train.GarageType.mode()[0])\n",
    "house_train.GarageYrBlt = house_train.GarageYrBlt.fillna(house_train.GarageYrBlt.mean())\n",
    "house_train.GarageFinish = house_train.GarageFinish.fillna(house_train.GarageFinish.mode()[0])\n",
    "house_train.GarageQual = house_train.GarageQual.fillna(house_train.GarageQual.mode()[0])\n",
    "house_train.GarageCond = house_train.GarageCond.fillna(house_train.GarageCond.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para *house_train*: Si la variable es categorica, los nulos se remplazan con el valor **mas repetido**.\n",
    "                        Si es numerica, los nulos se remplazan con la **media**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_test.LotFrontage = house_test.LotFrontage.fillna(house_test.LotFrontage.mean())\n",
    "house_test.MasVnrType = house_test.MasVnrType.fillna(house_test.MasVnrType.mode()[0])\n",
    "house_test.MasVnrArea = house_test.MasVnrArea.fillna(house_test.MasVnrArea.mean())\n",
    "house_test.BsmtQual = house_test.BsmtQual.fillna(house_test.BsmtQual.mode()[0])\n",
    "house_test.BsmtCond = house_test.BsmtCond.fillna(house_test.BsmtCond.mode()[0])\n",
    "house_test.BsmtExposure = house_test.BsmtExposure.fillna(house_test.BsmtExposure.mode()[0])\n",
    "house_test.BsmtFinType1 = house_test.BsmtFinType1.fillna(house_test.BsmtFinType1.mode()[0])\n",
    "house_test.BsmtFinType2 = house_test.BsmtFinType2.fillna(house_test.BsmtFinType2.mode()[0])\n",
    "house_test.Electrical = house_test.Electrical.fillna(house_test.Electrical.mode()[0])\n",
    "house_test.FireplaceQu = house_test.FireplaceQu.fillna(house_test.FireplaceQu.mode()[0])\n",
    "house_test.GarageType = house_test.GarageType.fillna(house_test.GarageType.mode()[0])\n",
    "house_test.GarageYrBlt = house_test.GarageYrBlt.fillna(house_test.GarageYrBlt.mean())\n",
    "house_test.GarageFinish = house_test.GarageFinish.fillna(house_test.GarageFinish.mode()[0])\n",
    "house_test.GarageQual = house_test.GarageQual.fillna(house_test.GarageQual.mode()[0])\n",
    "house_test.GarageCond = house_test.GarageCond.fillna(house_test.GarageCond.mode()[0])\n",
    "\n",
    "house_test.MSZoning = house_test.MSZoning.fillna(house_test.MSZoning.mode()[0])\n",
    "house_test.Utilities = house_test.Utilities.fillna(house_test.Utilities.mode()[0])\n",
    "house_test.Exterior1st = house_test.Exterior1st.fillna(house_test.Exterior1st.mode()[0])\n",
    "house_test.Exterior2nd = house_test.Exterior2nd.fillna(house_test.Exterior2nd.mode()[0])\n",
    "house_test.BsmtFinSF1 = house_test.BsmtFinSF1.fillna(house_test.BsmtFinSF1.mean())\n",
    "house_test.BsmtFinSF2 = house_test.BsmtFinSF2.fillna(house_test.BsmtFinSF2.mean())\n",
    "house_test.BsmtUnfSF = house_test.BsmtUnfSF.fillna(house_test.BsmtUnfSF.mean())\n",
    "house_test.TotalBsmtSF = house_test.TotalBsmtSF.fillna(house_test.TotalBsmtSF.mean())\n",
    "house_test.BsmtFullBath = house_test.BsmtFullBath.fillna(house_test.BsmtFullBath.mean())\n",
    "house_test.BsmtHalfBath = house_test.BsmtHalfBath.fillna(house_test.BsmtHalfBath.mean())\n",
    "house_test.KitchenQual = house_test.KitchenQual.fillna(house_test.KitchenQual.mode()[0])\n",
    "house_test.Functional = house_test.Functional.fillna(house_test.Functional.mode()[0])\n",
    "house_test.GarageCars = house_test.GarageCars.fillna(house_test.GarageCars.mean())\n",
    "house_test.GarageArea = house_test.GarageArea.fillna(house_test.GarageArea.mean())\n",
    "house_test.SaleType = house_test.SaleType.fillna(house_test.SaleType.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para *house_test*: Si la variable es categorica, los nulos se remplazan con el valor **mas repetido**.\n",
    "                        Si es numerica, los nulos se remplazan con la **media**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train.drop(['Id', 'Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\n",
    "house_test.drop(['Id', 'Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justificación de dropeo:\n",
    "- **Id:** No podria interactuar con el modelo\n",
    "- **Alley:** La suma de sus valores nulos supera el 50%\n",
    "- **PoolQC:** La suma de sus valores nulos supera el 50%\n",
    "- **Fence:** La suma de sus valores nulos supera el 50%\n",
    "- **MiscFeature:** La suma de sus valores nulos supera el 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns = house_train.columns[house_train.isna().any()].tolist()\n",
    "null_columns_type = {null_columns[i] : [house_train[null_columns[i]].isnull().sum(), str(house_train[null_columns[i]].dtype)] for i in range(len(null_columns))}\n",
    "null_columns_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns_test = house_test.columns[house_test.isna().any()].tolist()\n",
    "null_columns_type_test = {null_columns_test[i] : [house_test[null_columns_test[i]].isnull().sum(), str(house_test[null_columns_test[i]].dtype)] for i in range(len(null_columns_test))}\n",
    "null_columns_type_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Verificación** de valores nulos, para ambos DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 75)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = list(dict(house_train.select_dtypes(include=['object']).dtypes).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esta lista contiene el nombre de todas las columnas de tipo Object (*Categoricas*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= pd.concat([house_train, house_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justificación de concatenación:\n",
    "* Se previene el hecho de que en las variables categoricas de testeo, **existan más categorias** que en las de entrenamiento  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcolumns(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields], drop_first=True)\n",
    "\n",
    "        final_df.drop([fields], axis=1, inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            df_final=pd.concat([df_final, df1], axis=1)\n",
    "        i+=1\n",
    "    df_final=pd.concat([final_df, df_final], axis=1)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esta funcion, interactua con la lista de variables categoricas (de ambos DataSets) y **genera las correspondientes dummies** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 76)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estado actual de concatenación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition1\n",
      "Condition2\n",
      "BldgType\n",
      "HouseStyle\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "ExterQual\n",
      "ExterCond\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n",
      "SaleType\n",
      "SaleCondition\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcolumns(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 237)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estado posterior a obtener las dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.loc[:, ~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se excluyen las columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 177)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estado posterior a depuración de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>ConLI</th>\n",
       "      <th>ConLw</th>\n",
       "      <th>New</th>\n",
       "      <th>Oth</th>\n",
       "      <th>WD</th>\n",
       "      <th>AdjLand</th>\n",
       "      <th>Alloca</th>\n",
       "      <th>Family</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  ConLI  ConLw  New  \\\n",
       "0          2003       196.0       706.0         0.0  ...      0      0    0   \n",
       "1          1976         0.0       978.0         0.0  ...      0      0    0   \n",
       "2          2002       162.0       486.0         0.0  ...      0      0    0   \n",
       "3          1970         0.0       216.0         0.0  ...      0      0    0   \n",
       "4          2000       350.0       655.0         0.0  ...      0      0    0   \n",
       "\n",
       "   Oth  WD  AdjLand  Alloca  Family  Normal  Partial  \n",
       "0    0   1        0       0       0       1        0  \n",
       "1    0   1        0       0       0       1        0  \n",
       "2    0   1        0       0       0       1        0  \n",
       "3    0   1        0       0       0       0        0  \n",
       "4    0   1        0       0       0       1        0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=final_df.iloc[:1460, :]\n",
    "df_test=final_df.iloc[1460:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se divide la concatenación para devolver los DataSets de entrenamiento y testeo a su **forma** original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_7124\\3029927858.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop(['SalePrice'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_test.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eliminamos la variable dependiente del DataSet de testeo, porque inicialmente no lo tenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('df_train.parquet')\n",
    "df_test.to_parquet('df_test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exportamos nuestros DataSets normalizados y sus dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('df_train.parquet', 'pyarrow')\n",
    "df_test = pd.read_parquet('df_test.parquet', 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora podemos trabajar desde estas importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 176)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE DE PUESTA EN MARCHA DEL MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['SalePrice'], axis=1)\n",
    "y_train = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tomamos todas las variables independientes de las que disponemos como **X_train**\n",
    "* Establecemos el precio como variable dependiente o **y_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_analisys(y, y_pred, figsize=(10,4), title=''):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "    axs[0].scatter(y, y_pred)\n",
    "    mn = min(np.min(y), np.min(y_pred))\n",
    "    mx = max(np.max(y), np.max(y_pred))\n",
    "    axs[0].plot([mn, mx], [mn, mx], c='red')\n",
    "    axs[0].set_xlabel('$y$')\n",
    "    axs[0].set_ylabel('$\\hat{y}$')\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    evs = explained_variance_score(y, y_pred)\n",
    "    axs[0].set_title('rmse = {:.2f}, evs= {:.2f}'.format(rmse, evs))\n",
    "\n",
    "    axs[1].hist(y-y_pred, bins=50)\n",
    "    avg = np.mean(y-y_pred)\n",
    "    std = np.std(y-y_pred)\n",
    "    axs[1].set_xlabel('$y - \\hat{y}$')\n",
    "    axs[1].set_title('Histogram prediction error, $\\mu$ = {:.2f}, $\\sigma$ = {:.2f}'.format(avg, std))\n",
    "\n",
    "    fig.suptitle(title, fontweight =\"bold\", size=16, y=1.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esta función, dibuja por nosotros el esquema de dispersión de nuestra regresión y un histograma de errores, además, nos regresa los indicadores que necesitamos para evaluar el estado actual del modelo y su capacidad predictiva.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=13, max_features=0.2, max_samples=0.8,\n",
       "                      n_estimators=140)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=13, max_features=0.2, max_samples=0.8,\n",
       "                      n_estimators=140)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=13, max_features=0.2, max_samples=0.8,\n",
       "                      n_estimators=140)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "\n",
    "param_grid = {'n_estimators':np.arange(50,200,15),\n",
    "              'max_features':np.arange(0.1, 1, 0.1),\n",
    "              'max_depth': [5, 7, 9, 11, 13, 15],\n",
    "              'max_samples': [0.3, 0.5, 0.8]}\n",
    "\n",
    "model = RSCV(RandomForestRegressor(), param_grid, n_iter = 15).fit(X_train, y_train)\n",
    "model = model.best_estimator_\n",
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con este proceso se busca **determinar parametros optimos** para nuestra regresión apriori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;satandard_scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;lin_reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=15, n_estimators=500,\n",
       "                                       random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;satandard_scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;lin_reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=15, n_estimators=500,\n",
       "                                       random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=15, n_estimators=500, random_state=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('satandard_scaler', StandardScaler()),\n",
       "                ('lin_reg',\n",
       "                 RandomForestRegressor(max_depth=15, n_estimators=500,\n",
       "                                       random_state=0))])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lin = Pipeline((\n",
    "    ('satandard_scaler', StandardScaler()),\n",
    "    ('lin_reg', RandomForestRegressor(max_depth=15, random_state=0, n_estimators=500)),\n",
    "))\n",
    "\n",
    "model_lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred = model_lin.predict(X_train)\n",
    "\n",
    "plot_prediction_analisys(y_train, y_train_pred, title='Reg Model - Trainingset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justificación de estandarizacion:\n",
    "- La estandarización de un conjunto de datos es un requisito común para muchos estimadores de aprendizaje automático: es posible que se comporten mal si las características individuales no se parecen más o menos a datos estándar distribuidos normalmente (p. ej., gaussianos con media 0 y varianza unitaria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justificación de modelo:\n",
    "* Utilizamos RandomForestRegressor por las caracteristicas de nuestra variable dependiente, está es de tipo numerico (Responde a un precio), por tanto descartamos un modelo clasificatorio y se usa RandomForest por su capacidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('houses_model.pkl', 'wb') as pdis:\n",
    "    pickle.dump(model_lin, pdis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exportamos nuestro modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = pd.read_pickle('houses_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro error cuadratico medio y desviación estandar es: 10870.26\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "print('Nuestro error cuadratico medio y desviación estandar es: {:.2f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro error cuadratico medio logaritmico es 0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "rmsle = round(mean_squared_log_error(y_train, y_train_pred, squared=False), 2)\n",
    "print('Nuestro error cuadratico medio logaritmico es {:.2f}'.format(rmsle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE DE GENERACIÓN DE PRECIOS PARA LOS DATOS DE PRUEBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['SalePrice'] = model_lin.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['SalePrice'] = round(df_test['SalePrice'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usamos el modelo para generar los valores de precio en *house_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>ConLw</th>\n",
       "      <th>New</th>\n",
       "      <th>Oth</th>\n",
       "      <th>WD</th>\n",
       "      <th>AdjLand</th>\n",
       "      <th>Alloca</th>\n",
       "      <th>Family</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Partial</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126901.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155657.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>179576.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>182688.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>198609.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  ConLw  New  Oth  WD  \\\n",
       "0          1961         0.0       468.0       144.0  ...      0    0    0   1   \n",
       "1          1958       108.0       923.0         0.0  ...      0    0    0   1   \n",
       "2          1998         0.0       791.0         0.0  ...      0    0    0   1   \n",
       "3          1998        20.0       602.0         0.0  ...      0    0    0   1   \n",
       "4          1992         0.0       263.0         0.0  ...      0    0    0   1   \n",
       "\n",
       "   AdjLand  Alloca  Family  Normal  Partial  SalePrice  \n",
       "0        0       0       0       1        0  126901.33  \n",
       "1        0       0       0       1        0  155657.69  \n",
       "2        0       0       0       1        0  179576.61  \n",
       "3        0       0       0       1        0  182688.43  \n",
       "4        0       0       0       1        0  198609.68  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FASE FINAL CSV PARA HACER SUBMIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv('Housing Dreams/houses_test_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['SalePrice'] = df_test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = submit_df[['Id', 'SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>126901.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>155657.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>179576.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>182688.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>198609.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>84325.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>87334.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>153910.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>114876.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>226138.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SalePrice\n",
       "Id             \n",
       "1461  126901.33\n",
       "1462  155657.69\n",
       "1463  179576.61\n",
       "1464  182688.43\n",
       "1465  198609.68\n",
       "...         ...\n",
       "2915   84325.14\n",
       "2916   87334.05\n",
       "2917  153910.93\n",
       "2918  114876.97\n",
       "2919  226138.70\n",
       "\n",
       "[1459 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df.set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este nuevo DataSet, cuyo proposito es testear resultados, solo contiene el ID de la venta y su determinado precio (Producto de la predicción del modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con **'sample_submission.csv'**, se podra testear compárativamente los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 176)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU, PReLU, ELU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 2s 8ms/step - loss: 25408210944.0000 - val_loss: 5847259136.0000\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 6027282944.0000 - val_loss: 5046044672.0000\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 4217945344.0000 - val_loss: 4215028992.0000\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 3344432128.0000 - val_loss: 3852643328.0000\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 2837686016.0000 - val_loss: 3605779456.0000\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 2510976512.0000 - val_loss: 3410697472.0000\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 2183150080.0000 - val_loss: 3259324160.0000\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1912489984.0000 - val_loss: 3251845632.0000\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1776937600.0000 - val_loss: 3394961152.0000\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1684598528.0000 - val_loss: 3361225728.0000\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1661939456.0000 - val_loss: 3570347008.0000\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1638902016.0000 - val_loss: 3506770944.0000\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1614517376.0000 - val_loss: 3779226880.0000\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1614528640.0000 - val_loss: 3881412352.0000\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1609106688.0000 - val_loss: 3599369728.0000\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1599510400.0000 - val_loss: 3636804096.0000\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1594816256.0000 - val_loss: 4042235392.0000\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1576232192.0000 - val_loss: 3690470912.0000\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1574510720.0000 - val_loss: 3767848704.0000\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1591795840.0000 - val_loss: 3698003968.0000\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1575068416.0000 - val_loss: 3534822912.0000\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1537841920.0000 - val_loss: 3586274816.0000\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1547589632.0000 - val_loss: 3506058240.0000\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1536543360.0000 - val_loss: 3551364352.0000\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1583493632.0000 - val_loss: 3595815424.0000\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1565209984.0000 - val_loss: 3609527040.0000\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1551694208.0000 - val_loss: 3470144000.0000\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1526571392.0000 - val_loss: 3728493312.0000\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1509864960.0000 - val_loss: 3608705024.0000\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1517851264.0000 - val_loss: 3532547072.0000\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1529410560.0000 - val_loss: 3503710464.0000\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1516941056.0000 - val_loss: 3479891200.0000\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1499912832.0000 - val_loss: 3501362944.0000\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1481532416.0000 - val_loss: 3850695680.0000\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1490590208.0000 - val_loss: 3372170752.0000\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1504367616.0000 - val_loss: 3893048832.0000\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1523773696.0000 - val_loss: 3789888000.0000\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1479513472.0000 - val_loss: 3680419840.0000\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1458931200.0000 - val_loss: 3524561152.0000\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1487443456.0000 - val_loss: 3388647680.0000\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1490058496.0000 - val_loss: 3504933632.0000\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1509624064.0000 - val_loss: 3960262400.0000\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1455317248.0000 - val_loss: 3906524672.0000\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1468551168.0000 - val_loss: 3705936896.0000\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1437888768.0000 - val_loss: 4075415808.0000\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1458264576.0000 - val_loss: 3337028608.0000\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1448951936.0000 - val_loss: 3814062848.0000\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1428340096.0000 - val_loss: 3771043328.0000\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1447250048.0000 - val_loss: 3511545856.0000\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1426563840.0000 - val_loss: 3651517696.0000\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1468609152.0000 - val_loss: 3327319296.0000\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1427781376.0000 - val_loss: 3397241600.0000\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1499517824.0000 - val_loss: 3439219200.0000\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1428784512.0000 - val_loss: 3615835136.0000\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1407547776.0000 - val_loss: 3304641536.0000\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1456745600.0000 - val_loss: 3410453760.0000\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1416892288.0000 - val_loss: 3625050368.0000\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1497303296.0000 - val_loss: 3496351232.0000\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1392295296.0000 - val_loss: 3704022528.0000\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1451596416.0000 - val_loss: 4002380800.0000\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1422511488.0000 - val_loss: 3891032576.0000\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1403067776.0000 - val_loss: 3487793920.0000\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1396569856.0000 - val_loss: 3576999424.0000\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1348190336.0000 - val_loss: 3316290816.0000\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1420416640.0000 - val_loss: 3342753280.0000\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1393190144.0000 - val_loss: 3663032064.0000\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1395556480.0000 - val_loss: 3382176512.0000\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1386281344.0000 - val_loss: 3601737984.0000\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1415347968.0000 - val_loss: 3648919808.0000\n",
      "Epoch 70/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1385090176.0000 - val_loss: 3440939776.0000\n",
      "Epoch 71/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1423645696.0000 - val_loss: 4146040320.0000\n",
      "Epoch 72/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1375702528.0000 - val_loss: 3364540416.0000\n",
      "Epoch 73/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1370707712.0000 - val_loss: 3299094784.0000\n",
      "Epoch 74/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1371582848.0000 - val_loss: 3535876096.0000\n",
      "Epoch 75/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1333889280.0000 - val_loss: 3679553536.0000\n",
      "Epoch 76/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1330931200.0000 - val_loss: 3382221824.0000\n",
      "Epoch 77/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1330513792.0000 - val_loss: 3764005632.0000\n",
      "Epoch 78/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1356370048.0000 - val_loss: 3600913664.0000\n",
      "Epoch 79/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1354205696.0000 - val_loss: 3365872896.0000\n",
      "Epoch 80/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1327207424.0000 - val_loss: 3780141056.0000\n",
      "Epoch 81/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1353248768.0000 - val_loss: 3820283648.0000\n",
      "Epoch 82/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1355637888.0000 - val_loss: 3452857856.0000\n",
      "Epoch 83/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1359339392.0000 - val_loss: 3245383680.0000\n",
      "Epoch 84/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1303987712.0000 - val_loss: 3327905536.0000\n",
      "Epoch 85/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1294275328.0000 - val_loss: 3284594944.0000\n",
      "Epoch 86/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1325200640.0000 - val_loss: 3253830400.0000\n",
      "Epoch 87/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1343732352.0000 - val_loss: 3257877248.0000\n",
      "Epoch 88/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1300822016.0000 - val_loss: 3248728064.0000\n",
      "Epoch 89/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1308456704.0000 - val_loss: 3194909952.0000\n",
      "Epoch 90/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1301437952.0000 - val_loss: 3225748736.0000\n",
      "Epoch 91/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1311189120.0000 - val_loss: 3342160128.0000\n",
      "Epoch 92/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1345753728.0000 - val_loss: 3079799040.0000\n",
      "Epoch 93/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1322986496.0000 - val_loss: 3183057152.0000\n",
      "Epoch 94/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1313387392.0000 - val_loss: 3313574144.0000\n",
      "Epoch 95/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1305313920.0000 - val_loss: 3443962368.0000\n",
      "Epoch 96/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1306436096.0000 - val_loss: 3467938560.0000\n",
      "Epoch 97/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1263171712.0000 - val_loss: 3216868352.0000\n",
      "Epoch 98/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1266743424.0000 - val_loss: 3039405312.0000\n",
      "Epoch 99/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1283925120.0000 - val_loss: 2978664192.0000\n",
      "Epoch 100/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1293560064.0000 - val_loss: 3074199808.0000\n",
      "Epoch 101/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1266180608.0000 - val_loss: 3140385792.0000\n",
      "Epoch 102/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 1231575680.0000 - val_loss: 3471364608.0000\n",
      "Epoch 103/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1267654656.0000 - val_loss: 3178323712.0000\n",
      "Epoch 104/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1263287808.0000 - val_loss: 3224398080.0000\n",
      "Epoch 105/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1251316992.0000 - val_loss: 3029430784.0000\n",
      "Epoch 106/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1255546880.0000 - val_loss: 3073902336.0000\n",
      "Epoch 107/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1278317056.0000 - val_loss: 3181722112.0000\n",
      "Epoch 108/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1297363712.0000 - val_loss: 3065543168.0000\n",
      "Epoch 109/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1254144640.0000 - val_loss: 3266088960.0000\n",
      "Epoch 110/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1279341312.0000 - val_loss: 2988113664.0000\n",
      "Epoch 111/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1204099328.0000 - val_loss: 3142979328.0000\n",
      "Epoch 112/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1274604416.0000 - val_loss: 3535778560.0000\n",
      "Epoch 113/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1251441536.0000 - val_loss: 3065857280.0000\n",
      "Epoch 114/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 1259511936.0000 - val_loss: 3141253888.0000\n",
      "Epoch 115/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1244424704.0000 - val_loss: 3030717184.0000\n",
      "Epoch 116/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1297241856.0000 - val_loss: 3308067072.0000\n",
      "Epoch 117/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1214916352.0000 - val_loss: 2996554496.0000\n",
      "Epoch 118/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1208788352.0000 - val_loss: 3059314176.0000\n",
      "Epoch 119/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1263608448.0000 - val_loss: 3089811968.0000\n",
      "Epoch 120/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1227888640.0000 - val_loss: 3079761920.0000\n",
      "Epoch 121/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1237698688.0000 - val_loss: 3110811904.0000\n",
      "Epoch 122/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1193249280.0000 - val_loss: 3309539072.0000\n",
      "Epoch 123/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1243414272.0000 - val_loss: 3132241920.0000\n",
      "Epoch 124/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1208371200.0000 - val_loss: 3011870208.0000\n",
      "Epoch 125/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1212356352.0000 - val_loss: 3146876928.0000\n",
      "Epoch 126/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1208175744.0000 - val_loss: 2968677632.0000\n",
      "Epoch 127/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1212956928.0000 - val_loss: 2895133952.0000\n",
      "Epoch 128/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1217191040.0000 - val_loss: 2924895232.0000\n",
      "Epoch 129/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1187092480.0000 - val_loss: 3040544000.0000\n",
      "Epoch 130/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1205081984.0000 - val_loss: 2889587456.0000\n",
      "Epoch 131/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1223911168.0000 - val_loss: 3743557120.0000\n",
      "Epoch 132/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1189612288.0000 - val_loss: 2965228544.0000\n",
      "Epoch 133/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1192328192.0000 - val_loss: 3219868416.0000\n",
      "Epoch 134/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1209459456.0000 - val_loss: 3078250752.0000\n",
      "Epoch 135/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1194222080.0000 - val_loss: 2881393408.0000\n",
      "Epoch 136/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1188675072.0000 - val_loss: 3011033088.0000\n",
      "Epoch 137/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1172413952.0000 - val_loss: 2880505088.0000\n",
      "Epoch 138/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1169629056.0000 - val_loss: 2850813184.0000\n",
      "Epoch 139/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1157953024.0000 - val_loss: 2777084416.0000\n",
      "Epoch 140/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1198205184.0000 - val_loss: 2970880768.0000\n",
      "Epoch 141/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1204068480.0000 - val_loss: 2997971456.0000\n",
      "Epoch 142/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1175308416.0000 - val_loss: 2977658368.0000\n",
      "Epoch 143/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1143043712.0000 - val_loss: 3245770496.0000\n",
      "Epoch 144/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1166293120.0000 - val_loss: 2987817728.0000\n",
      "Epoch 145/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1169904640.0000 - val_loss: 3188456448.0000\n",
      "Epoch 146/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1168761984.0000 - val_loss: 2857542912.0000\n",
      "Epoch 147/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1151903104.0000 - val_loss: 2787970560.0000\n",
      "Epoch 148/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1175028992.0000 - val_loss: 2832736768.0000\n",
      "Epoch 149/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1165743104.0000 - val_loss: 2939478784.0000\n",
      "Epoch 150/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1168809472.0000 - val_loss: 2825931520.0000\n",
      "Epoch 151/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1119030528.0000 - val_loss: 2757352704.0000\n",
      "Epoch 152/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1153913344.0000 - val_loss: 2959765504.0000\n",
      "Epoch 153/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1131226880.0000 - val_loss: 2761178112.0000\n",
      "Epoch 154/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1141274368.0000 - val_loss: 2848150784.0000\n",
      "Epoch 155/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1125743616.0000 - val_loss: 2926345984.0000\n",
      "Epoch 156/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1170480896.0000 - val_loss: 2729993984.0000\n",
      "Epoch 157/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1105218304.0000 - val_loss: 2733048576.0000\n",
      "Epoch 158/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1113672448.0000 - val_loss: 3126354688.0000\n",
      "Epoch 159/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1139088384.0000 - val_loss: 2674426880.0000\n",
      "Epoch 160/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1135607936.0000 - val_loss: 3404774912.0000\n",
      "Epoch 161/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1122437376.0000 - val_loss: 3114669312.0000\n",
      "Epoch 162/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1148811904.0000 - val_loss: 2571453952.0000\n",
      "Epoch 163/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1093062912.0000 - val_loss: 2820633600.0000\n",
      "Epoch 164/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1135004544.0000 - val_loss: 2616879872.0000\n",
      "Epoch 165/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1131645184.0000 - val_loss: 2707830272.0000\n",
      "Epoch 166/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1094878720.0000 - val_loss: 2578477312.0000\n",
      "Epoch 167/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1099497984.0000 - val_loss: 2612613376.0000\n",
      "Epoch 168/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1101127808.0000 - val_loss: 2578195456.0000\n",
      "Epoch 169/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1083432320.0000 - val_loss: 2966973952.0000\n",
      "Epoch 170/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1081411328.0000 - val_loss: 2801357312.0000\n",
      "Epoch 171/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1088134912.0000 - val_loss: 2496869376.0000\n",
      "Epoch 172/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1065667072.0000 - val_loss: 2764598784.0000\n",
      "Epoch 173/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1063061568.0000 - val_loss: 2760638720.0000\n",
      "Epoch 174/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1085912064.0000 - val_loss: 2770416128.0000\n",
      "Epoch 175/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1066201024.0000 - val_loss: 2678351616.0000\n",
      "Epoch 176/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1081929088.0000 - val_loss: 2818607104.0000\n",
      "Epoch 177/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1094899584.0000 - val_loss: 2604521472.0000\n",
      "Epoch 178/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1093885952.0000 - val_loss: 2879982080.0000\n",
      "Epoch 179/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1059524864.0000 - val_loss: 2579939328.0000\n",
      "Epoch 180/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1058302848.0000 - val_loss: 2366442752.0000\n",
      "Epoch 181/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1059625664.0000 - val_loss: 2632228864.0000\n",
      "Epoch 182/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1077232640.0000 - val_loss: 2482961920.0000\n",
      "Epoch 183/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1029915328.0000 - val_loss: 2485851392.0000\n",
      "Epoch 184/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1068460928.0000 - val_loss: 2344285696.0000\n",
      "Epoch 185/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1041822208.0000 - val_loss: 2574343424.0000\n",
      "Epoch 186/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1009030272.0000 - val_loss: 2595900672.0000\n",
      "Epoch 187/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1006483904.0000 - val_loss: 2385676800.0000\n",
      "Epoch 188/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1019510784.0000 - val_loss: 2696628224.0000\n",
      "Epoch 189/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1007031552.0000 - val_loss: 2821091840.0000\n",
      "Epoch 190/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 1036012032.0000 - val_loss: 2599443200.0000\n",
      "Epoch 191/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1021295488.0000 - val_loss: 2414181632.0000\n",
      "Epoch 192/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 982383168.0000 - val_loss: 2330966016.0000\n",
      "Epoch 193/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 994063936.0000 - val_loss: 2366072832.0000\n",
      "Epoch 194/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1025611456.0000 - val_loss: 2716595456.0000\n",
      "Epoch 195/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1047171264.0000 - val_loss: 2277293312.0000\n",
      "Epoch 196/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 982690432.0000 - val_loss: 2282190592.0000\n",
      "Epoch 197/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 999585728.0000 - val_loss: 2238763008.0000\n",
      "Epoch 198/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 992248832.0000 - val_loss: 2084873472.0000\n",
      "Epoch 199/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1002021184.0000 - val_loss: 2757661696.0000\n",
      "Epoch 200/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1009449408.0000 - val_loss: 2082506112.0000\n",
      "Epoch 201/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 1002771840.0000 - val_loss: 2157462528.0000\n",
      "Epoch 202/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1018674624.0000 - val_loss: 2016593920.0000\n",
      "Epoch 203/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 935215232.0000 - val_loss: 2360893952.0000\n",
      "Epoch 204/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 957312832.0000 - val_loss: 2192027136.0000\n",
      "Epoch 205/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 957208064.0000 - val_loss: 2131273984.0000\n",
      "Epoch 206/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 952571008.0000 - val_loss: 2043864960.0000\n",
      "Epoch 207/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 922591680.0000 - val_loss: 2186694144.0000\n",
      "Epoch 208/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 970348992.0000 - val_loss: 2500144384.0000\n",
      "Epoch 209/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 920449856.0000 - val_loss: 2578689536.0000\n",
      "Epoch 210/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 932143872.0000 - val_loss: 2237746688.0000\n",
      "Epoch 211/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 985600960.0000 - val_loss: 2056850304.0000\n",
      "Epoch 212/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 894850752.0000 - val_loss: 2339099136.0000\n",
      "Epoch 213/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 919582784.0000 - val_loss: 1934471168.0000\n",
      "Epoch 214/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 892636352.0000 - val_loss: 2127648512.0000\n",
      "Epoch 215/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 907290176.0000 - val_loss: 2130407936.0000\n",
      "Epoch 216/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 932143744.0000 - val_loss: 2189154560.0000\n",
      "Epoch 217/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 928174464.0000 - val_loss: 2013322496.0000\n",
      "Epoch 218/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 872072128.0000 - val_loss: 2022227328.0000\n",
      "Epoch 219/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 903506944.0000 - val_loss: 1956541184.0000\n",
      "Epoch 220/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 934084096.0000 - val_loss: 1819645056.0000\n",
      "Epoch 221/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 863356992.0000 - val_loss: 1889167616.0000\n",
      "Epoch 222/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 866320576.0000 - val_loss: 2092659200.0000\n",
      "Epoch 223/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 846997312.0000 - val_loss: 1694047104.0000\n",
      "Epoch 224/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 865870976.0000 - val_loss: 1857350016.0000\n",
      "Epoch 225/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 838854400.0000 - val_loss: 1724625152.0000\n",
      "Epoch 226/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 868958592.0000 - val_loss: 1935972736.0000\n",
      "Epoch 227/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 837138240.0000 - val_loss: 1645366784.0000\n",
      "Epoch 228/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 844604032.0000 - val_loss: 1707876480.0000\n",
      "Epoch 229/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 833004672.0000 - val_loss: 1853590144.0000\n",
      "Epoch 230/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 834476608.0000 - val_loss: 1745876736.0000\n",
      "Epoch 231/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 802735040.0000 - val_loss: 2264558592.0000\n",
      "Epoch 232/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 878014272.0000 - val_loss: 1637916160.0000\n",
      "Epoch 233/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 820517056.0000 - val_loss: 1643769088.0000\n",
      "Epoch 234/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 810484416.0000 - val_loss: 1576636672.0000\n",
      "Epoch 235/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 786075904.0000 - val_loss: 1589795328.0000\n",
      "Epoch 236/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 800636224.0000 - val_loss: 1564109952.0000\n",
      "Epoch 237/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 783624448.0000 - val_loss: 1704967680.0000\n",
      "Epoch 238/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 773781056.0000 - val_loss: 1561644032.0000\n",
      "Epoch 239/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 776695552.0000 - val_loss: 1463249408.0000\n",
      "Epoch 240/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 799569856.0000 - val_loss: 1412435200.0000\n",
      "Epoch 241/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 744214784.0000 - val_loss: 1742440320.0000\n",
      "Epoch 242/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 759992576.0000 - val_loss: 1777208192.0000\n",
      "Epoch 243/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 737618432.0000 - val_loss: 1621300480.0000\n",
      "Epoch 244/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 740295680.0000 - val_loss: 1368224640.0000\n",
      "Epoch 245/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 742381568.0000 - val_loss: 1503817984.0000\n",
      "Epoch 246/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 716768512.0000 - val_loss: 1449348480.0000\n",
      "Epoch 247/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 719715136.0000 - val_loss: 1442158592.0000\n",
      "Epoch 248/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 694508224.0000 - val_loss: 1558901376.0000\n",
      "Epoch 249/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 719166656.0000 - val_loss: 1416731520.0000\n",
      "Epoch 250/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 707167360.0000 - val_loss: 1461326208.0000\n",
      "Epoch 251/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 709937664.0000 - val_loss: 1504258176.0000\n",
      "Epoch 252/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 748596288.0000 - val_loss: 1333997056.0000\n",
      "Epoch 253/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 707987648.0000 - val_loss: 1423212800.0000\n",
      "Epoch 254/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 728539008.0000 - val_loss: 1327088000.0000\n",
      "Epoch 255/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 677084288.0000 - val_loss: 1427251456.0000\n",
      "Epoch 256/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 737451328.0000 - val_loss: 1432151424.0000\n",
      "Epoch 257/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 677000320.0000 - val_loss: 1399791104.0000\n",
      "Epoch 258/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 680249344.0000 - val_loss: 1687688704.0000\n",
      "Epoch 259/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 677734656.0000 - val_loss: 1384330368.0000\n",
      "Epoch 260/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 659761408.0000 - val_loss: 1329930368.0000\n",
      "Epoch 261/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 654447296.0000 - val_loss: 1357050752.0000\n",
      "Epoch 262/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 626949888.0000 - val_loss: 1336265216.0000\n",
      "Epoch 263/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 663298240.0000 - val_loss: 1297595008.0000\n",
      "Epoch 264/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 645128704.0000 - val_loss: 1228035456.0000\n",
      "Epoch 265/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 738239744.0000 - val_loss: 1592916864.0000\n",
      "Epoch 266/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 830653504.0000 - val_loss: 1329457024.0000\n",
      "Epoch 267/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 619123904.0000 - val_loss: 1256903936.0000\n",
      "Epoch 268/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 640378688.0000 - val_loss: 1281478912.0000\n",
      "Epoch 269/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 621361152.0000 - val_loss: 1300364928.0000\n",
      "Epoch 270/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 632768832.0000 - val_loss: 1269952384.0000\n",
      "Epoch 271/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 596779648.0000 - val_loss: 1348946816.0000\n",
      "Epoch 272/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 600779200.0000 - val_loss: 1337236352.0000\n",
      "Epoch 273/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 618140352.0000 - val_loss: 1286041728.0000\n",
      "Epoch 274/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 602066240.0000 - val_loss: 1303875200.0000\n",
      "Epoch 275/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 609605952.0000 - val_loss: 1484930816.0000\n",
      "Epoch 276/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 639234624.0000 - val_loss: 1269444736.0000\n",
      "Epoch 277/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 629055744.0000 - val_loss: 1225816576.0000\n",
      "Epoch 278/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 586486208.0000 - val_loss: 1351670912.0000\n",
      "Epoch 279/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 636712256.0000 - val_loss: 1264915840.0000\n",
      "Epoch 280/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 659603712.0000 - val_loss: 1278422656.0000\n",
      "Epoch 281/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 575886784.0000 - val_loss: 1357514496.0000\n",
      "Epoch 282/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 597094656.0000 - val_loss: 1245254144.0000\n",
      "Epoch 283/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 586828224.0000 - val_loss: 1220656512.0000\n",
      "Epoch 284/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 602349312.0000 - val_loss: 1224642688.0000\n",
      "Epoch 285/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 580151296.0000 - val_loss: 1263835392.0000\n",
      "Epoch 286/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 588918016.0000 - val_loss: 1220966016.0000\n",
      "Epoch 287/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 547963328.0000 - val_loss: 1318443520.0000\n",
      "Epoch 288/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 569039488.0000 - val_loss: 1244188672.0000\n",
      "Epoch 289/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 545699392.0000 - val_loss: 1339041920.0000\n",
      "Epoch 290/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 571639168.0000 - val_loss: 1283234432.0000\n",
      "Epoch 291/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 529991328.0000 - val_loss: 1268552064.0000\n",
      "Epoch 292/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 566476736.0000 - val_loss: 1221827456.0000\n",
      "Epoch 293/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 542753664.0000 - val_loss: 1205920000.0000\n",
      "Epoch 294/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 581545024.0000 - val_loss: 1451173120.0000\n",
      "Epoch 295/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 559350592.0000 - val_loss: 1305947264.0000\n",
      "Epoch 296/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 562565760.0000 - val_loss: 1199063552.0000\n",
      "Epoch 297/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 613488576.0000 - val_loss: 1194323072.0000\n",
      "Epoch 298/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 571464576.0000 - val_loss: 1238495488.0000\n",
      "Epoch 299/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 577511680.0000 - val_loss: 1375234304.0000\n",
      "Epoch 300/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 513656288.0000 - val_loss: 1254069888.0000\n",
      "Epoch 301/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 502035776.0000 - val_loss: 1192209408.0000\n",
      "Epoch 302/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 504549664.0000 - val_loss: 1250967680.0000\n",
      "Epoch 303/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 561094400.0000 - val_loss: 1419098880.0000\n",
      "Epoch 304/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 531248960.0000 - val_loss: 1287923072.0000\n",
      "Epoch 305/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 508898432.0000 - val_loss: 1187783552.0000\n",
      "Epoch 306/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 520084480.0000 - val_loss: 1261588224.0000\n",
      "Epoch 307/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 507013152.0000 - val_loss: 1204460288.0000\n",
      "Epoch 308/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 599218048.0000 - val_loss: 1169676928.0000\n",
      "Epoch 309/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 503224224.0000 - val_loss: 1303144576.0000\n",
      "Epoch 310/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 496149312.0000 - val_loss: 1213549824.0000\n",
      "Epoch 311/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 496307168.0000 - val_loss: 1214712320.0000\n",
      "Epoch 312/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 506528576.0000 - val_loss: 1282440832.0000\n",
      "Epoch 313/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 648941248.0000 - val_loss: 1246700672.0000\n",
      "Epoch 314/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 490023616.0000 - val_loss: 1177820544.0000\n",
      "Epoch 315/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 522683552.0000 - val_loss: 1198066048.0000\n",
      "Epoch 316/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 491590432.0000 - val_loss: 1189361920.0000\n",
      "Epoch 317/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 494836352.0000 - val_loss: 1224393216.0000\n",
      "Epoch 318/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 497358528.0000 - val_loss: 1205579776.0000\n",
      "Epoch 319/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 568399424.0000 - val_loss: 1267358336.0000\n",
      "Epoch 320/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 514305824.0000 - val_loss: 1240440960.0000\n",
      "Epoch 321/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 480656000.0000 - val_loss: 1187008512.0000\n",
      "Epoch 322/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 534314400.0000 - val_loss: 1484034816.0000\n",
      "Epoch 323/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 554152192.0000 - val_loss: 1173309312.0000\n",
      "Epoch 324/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 487061696.0000 - val_loss: 1239607680.0000\n",
      "Epoch 325/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 485199520.0000 - val_loss: 1289095296.0000\n",
      "Epoch 326/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 533189952.0000 - val_loss: 1206230016.0000\n",
      "Epoch 327/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 499107712.0000 - val_loss: 1385613440.0000\n",
      "Epoch 328/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 474325952.0000 - val_loss: 1361374208.0000\n",
      "Epoch 329/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 524764096.0000 - val_loss: 1197976064.0000\n",
      "Epoch 330/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 480215168.0000 - val_loss: 1232805760.0000\n",
      "Epoch 331/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 478484576.0000 - val_loss: 1279814400.0000\n",
      "Epoch 332/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 547573760.0000 - val_loss: 1221259776.0000\n",
      "Epoch 333/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 531268608.0000 - val_loss: 1353378176.0000\n",
      "Epoch 334/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 480272832.0000 - val_loss: 1292606464.0000\n",
      "Epoch 335/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 570155200.0000 - val_loss: 1232525952.0000\n",
      "Epoch 336/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 479424736.0000 - val_loss: 1323734784.0000\n",
      "Epoch 337/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 472518784.0000 - val_loss: 1143768320.0000\n",
      "Epoch 338/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 480158592.0000 - val_loss: 1168097152.0000\n",
      "Epoch 339/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 466708192.0000 - val_loss: 1169162368.0000\n",
      "Epoch 340/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 505906624.0000 - val_loss: 1199034368.0000\n",
      "Epoch 341/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 455301376.0000 - val_loss: 1257077504.0000\n",
      "Epoch 342/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 488364064.0000 - val_loss: 1231511680.0000\n",
      "Epoch 343/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 522285056.0000 - val_loss: 1485582848.0000\n",
      "Epoch 344/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 488198304.0000 - val_loss: 1142102912.0000\n",
      "Epoch 345/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 489035872.0000 - val_loss: 1166972672.0000\n",
      "Epoch 346/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 445425984.0000 - val_loss: 1179871104.0000\n",
      "Epoch 347/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 451790176.0000 - val_loss: 1245992832.0000\n",
      "Epoch 348/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 475014912.0000 - val_loss: 1194553984.0000\n",
      "Epoch 349/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 449260288.0000 - val_loss: 1227993728.0000\n",
      "Epoch 350/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 487933760.0000 - val_loss: 1141581184.0000\n",
      "Epoch 351/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 471038432.0000 - val_loss: 1177662592.0000\n",
      "Epoch 352/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 505289920.0000 - val_loss: 1169293952.0000\n",
      "Epoch 353/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 432108608.0000 - val_loss: 1142161408.0000\n",
      "Epoch 354/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 436497408.0000 - val_loss: 1142418560.0000\n",
      "Epoch 355/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 437350240.0000 - val_loss: 1138731648.0000\n",
      "Epoch 356/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 424634816.0000 - val_loss: 1198386048.0000\n",
      "Epoch 357/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 489344224.0000 - val_loss: 1249218560.0000\n",
      "Epoch 358/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 437787872.0000 - val_loss: 1159872384.0000\n",
      "Epoch 359/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 484149152.0000 - val_loss: 1136524544.0000\n",
      "Epoch 360/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 475867776.0000 - val_loss: 1119130496.0000\n",
      "Epoch 361/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 499484032.0000 - val_loss: 1084202112.0000\n",
      "Epoch 362/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 420652928.0000 - val_loss: 1344258560.0000\n",
      "Epoch 363/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 456268448.0000 - val_loss: 1164905728.0000\n",
      "Epoch 364/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 472569952.0000 - val_loss: 1197020544.0000\n",
      "Epoch 365/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 436216512.0000 - val_loss: 1163943168.0000\n",
      "Epoch 366/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 415081760.0000 - val_loss: 1167218432.0000\n",
      "Epoch 367/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 496084064.0000 - val_loss: 1151647872.0000\n",
      "Epoch 368/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 414566400.0000 - val_loss: 1164733952.0000\n",
      "Epoch 369/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 420469792.0000 - val_loss: 1159065088.0000\n",
      "Epoch 370/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 447054432.0000 - val_loss: 1261109376.0000\n",
      "Epoch 371/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 433441600.0000 - val_loss: 1420510592.0000\n",
      "Epoch 372/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 437995296.0000 - val_loss: 1158109824.0000\n",
      "Epoch 373/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 410249056.0000 - val_loss: 1149420288.0000\n",
      "Epoch 374/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 412584736.0000 - val_loss: 1338744320.0000\n",
      "Epoch 375/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 421921888.0000 - val_loss: 1144229888.0000\n",
      "Epoch 376/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 416009056.0000 - val_loss: 1121140992.0000\n",
      "Epoch 377/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 423846496.0000 - val_loss: 1146546944.0000\n",
      "Epoch 378/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 421608384.0000 - val_loss: 1433448064.0000\n",
      "Epoch 379/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 455241248.0000 - val_loss: 1204391296.0000\n",
      "Epoch 380/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 407459584.0000 - val_loss: 1329351808.0000\n",
      "Epoch 381/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 572278784.0000 - val_loss: 1296434560.0000\n",
      "Epoch 382/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 445095808.0000 - val_loss: 1189656832.0000\n",
      "Epoch 383/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 443358112.0000 - val_loss: 1292178176.0000\n",
      "Epoch 384/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 435212256.0000 - val_loss: 1217330176.0000\n",
      "Epoch 385/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 401966240.0000 - val_loss: 1188865152.0000\n",
      "Epoch 386/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 420860192.0000 - val_loss: 1512587008.0000\n",
      "Epoch 387/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 408072544.0000 - val_loss: 1187003904.0000\n",
      "Epoch 388/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 398607168.0000 - val_loss: 1218519424.0000\n",
      "Epoch 389/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 481902976.0000 - val_loss: 1115790592.0000\n",
      "Epoch 390/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 447719776.0000 - val_loss: 1234461824.0000\n",
      "Epoch 391/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 403322240.0000 - val_loss: 1165795712.0000\n",
      "Epoch 392/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 409450336.0000 - val_loss: 1214677248.0000\n",
      "Epoch 393/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 435542880.0000 - val_loss: 1446248832.0000\n",
      "Epoch 394/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 599711616.0000 - val_loss: 1319823488.0000\n",
      "Epoch 395/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 403619264.0000 - val_loss: 1165662336.0000\n",
      "Epoch 396/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 427902368.0000 - val_loss: 1844387584.0000\n",
      "Epoch 397/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 505908640.0000 - val_loss: 1504123904.0000\n",
      "Epoch 398/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 429226016.0000 - val_loss: 1110882048.0000\n",
      "Epoch 399/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 422666912.0000 - val_loss: 1242867712.0000\n",
      "Epoch 400/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 381383392.0000 - val_loss: 1126764032.0000\n",
      "Epoch 401/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 379967744.0000 - val_loss: 1210710144.0000\n",
      "Epoch 402/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 390166880.0000 - val_loss: 1232461312.0000\n",
      "Epoch 403/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 446793056.0000 - val_loss: 1196807808.0000\n",
      "Epoch 404/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 380015552.0000 - val_loss: 1247425664.0000\n",
      "Epoch 405/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 419652320.0000 - val_loss: 1189821952.0000\n",
      "Epoch 406/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 370579328.0000 - val_loss: 1129669120.0000\n",
      "Epoch 407/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 451340128.0000 - val_loss: 1381086080.0000\n",
      "Epoch 408/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 378464384.0000 - val_loss: 1205908352.0000\n",
      "Epoch 409/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 418456224.0000 - val_loss: 1157244160.0000\n",
      "Epoch 410/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 381706016.0000 - val_loss: 1149805952.0000\n",
      "Epoch 411/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 380385824.0000 - val_loss: 1248743296.0000\n",
      "Epoch 412/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 405962240.0000 - val_loss: 1248153344.0000\n",
      "Epoch 413/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 405883328.0000 - val_loss: 1241376128.0000\n",
      "Epoch 414/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 407714624.0000 - val_loss: 1195107072.0000\n",
      "Epoch 415/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 376733312.0000 - val_loss: 1184716544.0000\n",
      "Epoch 416/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 475508064.0000 - val_loss: 1283172480.0000\n",
      "Epoch 417/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 379655392.0000 - val_loss: 1202263424.0000\n",
      "Epoch 418/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 393221952.0000 - val_loss: 1333646464.0000\n",
      "Epoch 419/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 412758688.0000 - val_loss: 1314726528.0000\n",
      "Epoch 420/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 399891424.0000 - val_loss: 1145758080.0000\n",
      "Epoch 421/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 371031264.0000 - val_loss: 1257828352.0000\n",
      "Epoch 422/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 394510144.0000 - val_loss: 1235819136.0000\n",
      "Epoch 423/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 362297088.0000 - val_loss: 1138641408.0000\n",
      "Epoch 424/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 371459840.0000 - val_loss: 1247173376.0000\n",
      "Epoch 425/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 356742880.0000 - val_loss: 1216915456.0000\n",
      "Epoch 426/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 636378816.0000 - val_loss: 1067468544.0000\n",
      "Epoch 427/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 388639264.0000 - val_loss: 1194695552.0000\n",
      "Epoch 428/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 355922048.0000 - val_loss: 1215027200.0000\n",
      "Epoch 429/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 394636832.0000 - val_loss: 1317417728.0000\n",
      "Epoch 430/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 355452704.0000 - val_loss: 1222357888.0000\n",
      "Epoch 431/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 360988512.0000 - val_loss: 1153460736.0000\n",
      "Epoch 432/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 368178752.0000 - val_loss: 1174174080.0000\n",
      "Epoch 433/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 362382432.0000 - val_loss: 1262176512.0000\n",
      "Epoch 434/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 359741184.0000 - val_loss: 1218604800.0000\n",
      "Epoch 435/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 428494720.0000 - val_loss: 1151480448.0000\n",
      "Epoch 436/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 370207520.0000 - val_loss: 1209766784.0000\n",
      "Epoch 437/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 382229504.0000 - val_loss: 1096681728.0000\n",
      "Epoch 438/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 364541184.0000 - val_loss: 1167632896.0000\n",
      "Epoch 439/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 387475040.0000 - val_loss: 1242569088.0000\n",
      "Epoch 440/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 367250944.0000 - val_loss: 1190390912.0000\n",
      "Epoch 441/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 405055136.0000 - val_loss: 1253500288.0000\n",
      "Epoch 442/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 386357344.0000 - val_loss: 1232210560.0000\n",
      "Epoch 443/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 356303776.0000 - val_loss: 1137017088.0000\n",
      "Epoch 444/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 344964000.0000 - val_loss: 1303998464.0000\n",
      "Epoch 445/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 363834816.0000 - val_loss: 1316854016.0000\n",
      "Epoch 446/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 356630368.0000 - val_loss: 1544154880.0000\n",
      "Epoch 447/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 362976768.0000 - val_loss: 1167380480.0000\n",
      "Epoch 448/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 370543008.0000 - val_loss: 1271012224.0000\n",
      "Epoch 449/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 501099264.0000 - val_loss: 1019873600.0000\n",
      "Epoch 450/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 361517760.0000 - val_loss: 1187200640.0000\n",
      "Epoch 451/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 358291328.0000 - val_loss: 1350572032.0000\n",
      "Epoch 452/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 334749248.0000 - val_loss: 1300948352.0000\n",
      "Epoch 453/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 343029248.0000 - val_loss: 1171579648.0000\n",
      "Epoch 454/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 363787584.0000 - val_loss: 1283523712.0000\n",
      "Epoch 455/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 354063424.0000 - val_loss: 1243921664.0000\n",
      "Epoch 456/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 348408832.0000 - val_loss: 1239741056.0000\n",
      "Epoch 457/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 409248992.0000 - val_loss: 1186000896.0000\n",
      "Epoch 458/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 342347872.0000 - val_loss: 1272423552.0000\n",
      "Epoch 459/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 336704704.0000 - val_loss: 1190101120.0000\n",
      "Epoch 460/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 352504896.0000 - val_loss: 1321501696.0000\n",
      "Epoch 461/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 384412640.0000 - val_loss: 1296857600.0000\n",
      "Epoch 462/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 339948640.0000 - val_loss: 1191745792.0000\n",
      "Epoch 463/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 362255808.0000 - val_loss: 1142214784.0000\n",
      "Epoch 464/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 346177472.0000 - val_loss: 1207576832.0000\n",
      "Epoch 465/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 351067232.0000 - val_loss: 1173098496.0000\n",
      "Epoch 466/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 344973600.0000 - val_loss: 1250962816.0000\n",
      "Epoch 467/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 374306304.0000 - val_loss: 1201520128.0000\n",
      "Epoch 468/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 335135840.0000 - val_loss: 1225181440.0000\n",
      "Epoch 469/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 359183712.0000 - val_loss: 1183349888.0000\n",
      "Epoch 470/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 405884288.0000 - val_loss: 1247871360.0000\n",
      "Epoch 471/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 407992544.0000 - val_loss: 1610708864.0000\n",
      "Epoch 472/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 407980384.0000 - val_loss: 1624035456.0000\n",
      "Epoch 473/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 348481472.0000 - val_loss: 1178299648.0000\n",
      "Epoch 474/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 335769728.0000 - val_loss: 1540268672.0000\n",
      "Epoch 475/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 392202944.0000 - val_loss: 1174120832.0000\n",
      "Epoch 476/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 341475904.0000 - val_loss: 1148536064.0000\n",
      "Epoch 477/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 350482560.0000 - val_loss: 1227670656.0000\n",
      "Epoch 478/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 338118560.0000 - val_loss: 1213526656.0000\n",
      "Epoch 479/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 365963200.0000 - val_loss: 1659826048.0000\n",
      "Epoch 480/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 358738336.0000 - val_loss: 1305268736.0000\n",
      "Epoch 481/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 335893056.0000 - val_loss: 1241261696.0000\n",
      "Epoch 482/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 366947872.0000 - val_loss: 1298830592.0000\n",
      "Epoch 483/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 374262432.0000 - val_loss: 1552705280.0000\n",
      "Epoch 484/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 331291200.0000 - val_loss: 1392743168.0000\n",
      "Epoch 485/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 367713728.0000 - val_loss: 1217443456.0000\n",
      "Epoch 486/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 323215616.0000 - val_loss: 1238490496.0000\n",
      "Epoch 487/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 326497312.0000 - val_loss: 1213172864.0000\n",
      "Epoch 488/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 343082144.0000 - val_loss: 1235246464.0000\n",
      "Epoch 489/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 346702560.0000 - val_loss: 1299464192.0000\n",
      "Epoch 490/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 375638912.0000 - val_loss: 1215698048.0000\n",
      "Epoch 491/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 334315936.0000 - val_loss: 1298112000.0000\n",
      "Epoch 492/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 346531360.0000 - val_loss: 1288849024.0000\n",
      "Epoch 493/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 329252416.0000 - val_loss: 1224727552.0000\n",
      "Epoch 494/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 332621504.0000 - val_loss: 1206415232.0000\n",
      "Epoch 495/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 352038624.0000 - val_loss: 1392179584.0000\n",
      "Epoch 496/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 334832864.0000 - val_loss: 1368976512.0000\n",
      "Epoch 497/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 326376704.0000 - val_loss: 1214563072.0000\n",
      "Epoch 498/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 326084992.0000 - val_loss: 1251266560.0000\n",
      "Epoch 499/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 408647008.0000 - val_loss: 1107933056.0000\n",
      "Epoch 500/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 365528800.0000 - val_loss: 1264462720.0000\n",
      "Epoch 501/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 329337504.0000 - val_loss: 1229850496.0000\n",
      "Epoch 502/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 325503776.0000 - val_loss: 1202790528.0000\n",
      "Epoch 503/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 330519104.0000 - val_loss: 1127101312.0000\n",
      "Epoch 504/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 369384992.0000 - val_loss: 1153470080.0000\n",
      "Epoch 505/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 393691200.0000 - val_loss: 2131569408.0000\n",
      "Epoch 506/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 337329792.0000 - val_loss: 1422905472.0000\n",
      "Epoch 507/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 416121280.0000 - val_loss: 1224087552.0000\n",
      "Epoch 508/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 333142560.0000 - val_loss: 1543987840.0000\n",
      "Epoch 509/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 342198208.0000 - val_loss: 1481196672.0000\n",
      "Epoch 510/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 312891968.0000 - val_loss: 1274090624.0000\n",
      "Epoch 511/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 325909024.0000 - val_loss: 1376013568.0000\n",
      "Epoch 512/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 332783424.0000 - val_loss: 1233054848.0000\n",
      "Epoch 513/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 332264928.0000 - val_loss: 1224640640.0000\n",
      "Epoch 514/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 353262720.0000 - val_loss: 2231320832.0000\n",
      "Epoch 515/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 403146496.0000 - val_loss: 1532466944.0000\n",
      "Epoch 516/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 319656352.0000 - val_loss: 1300710784.0000\n",
      "Epoch 517/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 314198400.0000 - val_loss: 1266227328.0000\n",
      "Epoch 518/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304282880.0000 - val_loss: 1208319616.0000\n",
      "Epoch 519/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 315298144.0000 - val_loss: 1386654208.0000\n",
      "Epoch 520/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 357525184.0000 - val_loss: 1284115328.0000\n",
      "Epoch 521/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 306116064.0000 - val_loss: 1371568640.0000\n",
      "Epoch 522/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 336815552.0000 - val_loss: 1481567360.0000\n",
      "Epoch 523/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 344310432.0000 - val_loss: 1351837824.0000\n",
      "Epoch 524/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 325347616.0000 - val_loss: 1580200576.0000\n",
      "Epoch 525/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 360307264.0000 - val_loss: 1411689600.0000\n",
      "Epoch 526/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 325081440.0000 - val_loss: 1133748480.0000\n",
      "Epoch 527/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 327827904.0000 - val_loss: 1533702272.0000\n",
      "Epoch 528/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 316106240.0000 - val_loss: 1493196288.0000\n",
      "Epoch 529/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 311746144.0000 - val_loss: 1380559232.0000\n",
      "Epoch 530/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 310935808.0000 - val_loss: 1241346944.0000\n",
      "Epoch 531/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 340930080.0000 - val_loss: 1624066944.0000\n",
      "Epoch 532/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 313734656.0000 - val_loss: 1280262912.0000\n",
      "Epoch 533/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 318958400.0000 - val_loss: 1305469696.0000\n",
      "Epoch 534/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 359547584.0000 - val_loss: 1233662208.0000\n",
      "Epoch 535/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 310496672.0000 - val_loss: 1276693632.0000\n",
      "Epoch 536/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 310452384.0000 - val_loss: 1131796992.0000\n",
      "Epoch 537/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 375509216.0000 - val_loss: 1607174400.0000\n",
      "Epoch 538/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 318637696.0000 - val_loss: 1222807040.0000\n",
      "Epoch 539/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 312994656.0000 - val_loss: 1252980480.0000\n",
      "Epoch 540/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 350972224.0000 - val_loss: 1296233600.0000\n",
      "Epoch 541/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 298297920.0000 - val_loss: 1292532608.0000\n",
      "Epoch 542/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 308676448.0000 - val_loss: 1261267584.0000\n",
      "Epoch 543/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 308083424.0000 - val_loss: 1383598720.0000\n",
      "Epoch 544/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 317225696.0000 - val_loss: 1253444864.0000\n",
      "Epoch 545/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 323470016.0000 - val_loss: 1369260160.0000\n",
      "Epoch 546/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304409440.0000 - val_loss: 1393464448.0000\n",
      "Epoch 547/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 311223392.0000 - val_loss: 1522180480.0000\n",
      "Epoch 548/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 330038496.0000 - val_loss: 1302689152.0000\n",
      "Epoch 549/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 319931552.0000 - val_loss: 1239497344.0000\n",
      "Epoch 550/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 338273184.0000 - val_loss: 1244308224.0000\n",
      "Epoch 551/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 289178400.0000 - val_loss: 1216027776.0000\n",
      "Epoch 552/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 302527488.0000 - val_loss: 1222918272.0000\n",
      "Epoch 553/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 328264320.0000 - val_loss: 1192140032.0000\n",
      "Epoch 554/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 350985088.0000 - val_loss: 1260387200.0000\n",
      "Epoch 555/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 320919680.0000 - val_loss: 1255558144.0000\n",
      "Epoch 556/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 311607360.0000 - val_loss: 1127792128.0000\n",
      "Epoch 557/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 322866656.0000 - val_loss: 1288512896.0000\n",
      "Epoch 558/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 339191456.0000 - val_loss: 1372659456.0000\n",
      "Epoch 559/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 335948832.0000 - val_loss: 1267887104.0000\n",
      "Epoch 560/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 305743936.0000 - val_loss: 1500981888.0000\n",
      "Epoch 561/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 316255200.0000 - val_loss: 1314872704.0000\n",
      "Epoch 562/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 291940320.0000 - val_loss: 1205063424.0000\n",
      "Epoch 563/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 294920352.0000 - val_loss: 1309807488.0000\n",
      "Epoch 564/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 305065408.0000 - val_loss: 1358722944.0000\n",
      "Epoch 565/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 339417664.0000 - val_loss: 1309179648.0000\n",
      "Epoch 566/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 320531936.0000 - val_loss: 1294958208.0000\n",
      "Epoch 567/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 313019680.0000 - val_loss: 1412545152.0000\n",
      "Epoch 568/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 332794368.0000 - val_loss: 1142022656.0000\n",
      "Epoch 569/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 299127712.0000 - val_loss: 1186089088.0000\n",
      "Epoch 570/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 290867776.0000 - val_loss: 1233253248.0000\n",
      "Epoch 571/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 667229056.0000 - val_loss: 993213248.0000\n",
      "Epoch 572/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 414928384.0000 - val_loss: 1510993920.0000\n",
      "Epoch 573/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 315078048.0000 - val_loss: 1237326720.0000\n",
      "Epoch 574/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 294072736.0000 - val_loss: 1238324096.0000\n",
      "Epoch 575/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 289911296.0000 - val_loss: 1400195328.0000\n",
      "Epoch 576/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 317516960.0000 - val_loss: 1483335296.0000\n",
      "Epoch 577/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 330939200.0000 - val_loss: 1390423808.0000\n",
      "Epoch 578/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304658368.0000 - val_loss: 1260578944.0000\n",
      "Epoch 579/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 303477120.0000 - val_loss: 1507184768.0000\n",
      "Epoch 580/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 314746144.0000 - val_loss: 1229795328.0000\n",
      "Epoch 581/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304125728.0000 - val_loss: 1296391296.0000\n",
      "Epoch 582/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 285875200.0000 - val_loss: 1200011264.0000\n",
      "Epoch 583/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 299829824.0000 - val_loss: 1191669632.0000\n",
      "Epoch 584/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 307644448.0000 - val_loss: 1372992896.0000\n",
      "Epoch 585/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 293858208.0000 - val_loss: 1216443136.0000\n",
      "Epoch 586/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 295194496.0000 - val_loss: 1119964544.0000\n",
      "Epoch 587/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 271944480.0000 - val_loss: 1186816128.0000\n",
      "Epoch 588/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 272917184.0000 - val_loss: 1365924352.0000\n",
      "Epoch 589/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 298523168.0000 - val_loss: 1215681280.0000\n",
      "Epoch 590/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 299541536.0000 - val_loss: 1345378688.0000\n",
      "Epoch 591/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 315381888.0000 - val_loss: 1456097664.0000\n",
      "Epoch 592/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 309600896.0000 - val_loss: 1195135488.0000\n",
      "Epoch 593/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 298357280.0000 - val_loss: 1174446720.0000\n",
      "Epoch 594/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 285129024.0000 - val_loss: 1305703936.0000\n",
      "Epoch 595/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 296218496.0000 - val_loss: 1395448704.0000\n",
      "Epoch 596/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 322905888.0000 - val_loss: 1281548800.0000\n",
      "Epoch 597/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 307567168.0000 - val_loss: 1166871296.0000\n",
      "Epoch 598/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304786944.0000 - val_loss: 1439152256.0000\n",
      "Epoch 599/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 283040096.0000 - val_loss: 1273768832.0000\n",
      "Epoch 600/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 290586016.0000 - val_loss: 1206612992.0000\n",
      "Epoch 601/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 285406432.0000 - val_loss: 1534949760.0000\n",
      "Epoch 602/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 318824160.0000 - val_loss: 1411973632.0000\n",
      "Epoch 603/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 298161824.0000 - val_loss: 1118203648.0000\n",
      "Epoch 604/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 304193344.0000 - val_loss: 1146617344.0000\n",
      "Epoch 605/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 275939456.0000 - val_loss: 1354923264.0000\n",
      "Epoch 606/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 283355712.0000 - val_loss: 1250280832.0000\n",
      "Epoch 607/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 325048352.0000 - val_loss: 1108873728.0000\n",
      "Epoch 608/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 329498752.0000 - val_loss: 1250975360.0000\n",
      "Epoch 609/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 295015424.0000 - val_loss: 1393967488.0000\n",
      "Epoch 610/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 280207552.0000 - val_loss: 1258243072.0000\n",
      "Epoch 611/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 308515296.0000 - val_loss: 1289656960.0000\n",
      "Epoch 612/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 282494976.0000 - val_loss: 1247837312.0000\n",
      "Epoch 613/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 267386880.0000 - val_loss: 1427204864.0000\n",
      "Epoch 614/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 292201184.0000 - val_loss: 1163094912.0000\n",
      "Epoch 615/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 273631616.0000 - val_loss: 1124550784.0000\n",
      "Epoch 616/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 290915552.0000 - val_loss: 1084130048.0000\n",
      "Epoch 617/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 430856384.0000 - val_loss: 1104383360.0000\n",
      "Epoch 618/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 324969248.0000 - val_loss: 1224956544.0000\n",
      "Epoch 619/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 352700256.0000 - val_loss: 1991570304.0000\n",
      "Epoch 620/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 321229120.0000 - val_loss: 1386959616.0000\n",
      "Epoch 621/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 297347776.0000 - val_loss: 1216081024.0000\n",
      "Epoch 622/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 278271968.0000 - val_loss: 1238906496.0000\n",
      "Epoch 623/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 265142304.0000 - val_loss: 1171055360.0000\n",
      "Epoch 624/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 326217056.0000 - val_loss: 1364401024.0000\n",
      "Epoch 625/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 288362112.0000 - val_loss: 1276720896.0000\n",
      "Epoch 626/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 259409408.0000 - val_loss: 1233986816.0000\n",
      "Epoch 627/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 282441888.0000 - val_loss: 1195349376.0000\n",
      "Epoch 628/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 285235136.0000 - val_loss: 1189180800.0000\n",
      "Epoch 629/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 264659456.0000 - val_loss: 1161583360.0000\n",
      "Epoch 630/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 269391072.0000 - val_loss: 1405751808.0000\n",
      "Epoch 631/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 291681088.0000 - val_loss: 1283471872.0000\n",
      "Epoch 632/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 271325984.0000 - val_loss: 1271537664.0000\n",
      "Epoch 633/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 285487264.0000 - val_loss: 1288662784.0000\n",
      "Epoch 634/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 277835168.0000 - val_loss: 1469749376.0000\n",
      "Epoch 635/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 289704256.0000 - val_loss: 1167057408.0000\n",
      "Epoch 636/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 300724800.0000 - val_loss: 1338516608.0000\n",
      "Epoch 637/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 258362080.0000 - val_loss: 1085130624.0000\n",
      "Epoch 638/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 320562144.0000 - val_loss: 1184028928.0000\n",
      "Epoch 639/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 282069824.0000 - val_loss: 1186369152.0000\n",
      "Epoch 640/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 290905696.0000 - val_loss: 1115956864.0000\n",
      "Epoch 641/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 292971776.0000 - val_loss: 1118576640.0000\n",
      "Epoch 642/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 364714496.0000 - val_loss: 1079998976.0000\n",
      "Epoch 643/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 289342464.0000 - val_loss: 1182388992.0000\n",
      "Epoch 644/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 296486752.0000 - val_loss: 1190607872.0000\n",
      "Epoch 645/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 259378608.0000 - val_loss: 1104058752.0000\n",
      "Epoch 646/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 287500928.0000 - val_loss: 1206612864.0000\n",
      "Epoch 647/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 329468160.0000 - val_loss: 1232856576.0000\n",
      "Epoch 648/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 271060832.0000 - val_loss: 1151696768.0000\n",
      "Epoch 649/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 307312160.0000 - val_loss: 1264891392.0000\n",
      "Epoch 650/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 264344032.0000 - val_loss: 1466224384.0000\n",
      "Epoch 651/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 269222272.0000 - val_loss: 1289373184.0000\n",
      "Epoch 652/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 289595616.0000 - val_loss: 1207458304.0000\n",
      "Epoch 653/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 270420864.0000 - val_loss: 1274250880.0000\n",
      "Epoch 654/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 264608752.0000 - val_loss: 1257672576.0000\n",
      "Epoch 655/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 296431488.0000 - val_loss: 1057566528.0000\n",
      "Epoch 656/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 298605472.0000 - val_loss: 1245474688.0000\n",
      "Epoch 657/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 283537056.0000 - val_loss: 1140203392.0000\n",
      "Epoch 658/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 290051264.0000 - val_loss: 1099915648.0000\n",
      "Epoch 659/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 256826976.0000 - val_loss: 1151513600.0000\n",
      "Epoch 660/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 260791840.0000 - val_loss: 1116260864.0000\n",
      "Epoch 661/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 313860544.0000 - val_loss: 1483590784.0000\n",
      "Epoch 662/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 269265728.0000 - val_loss: 1096107392.0000\n",
      "Epoch 663/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 267061328.0000 - val_loss: 1270848384.0000\n",
      "Epoch 664/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 259425184.0000 - val_loss: 1218276480.0000\n",
      "Epoch 665/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 277464416.0000 - val_loss: 1129393024.0000\n",
      "Epoch 666/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 296742400.0000 - val_loss: 1088815744.0000\n",
      "Epoch 667/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 290972416.0000 - val_loss: 1271609728.0000\n",
      "Epoch 668/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 344340704.0000 - val_loss: 1088296320.0000\n",
      "Epoch 669/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 277881216.0000 - val_loss: 1180411776.0000\n",
      "Epoch 670/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 277786368.0000 - val_loss: 1151563904.0000\n",
      "Epoch 671/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 278291680.0000 - val_loss: 1222061184.0000\n",
      "Epoch 672/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 270763616.0000 - val_loss: 1201821952.0000\n",
      "Epoch 673/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 331397376.0000 - val_loss: 1167003136.0000\n",
      "Epoch 674/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 274154816.0000 - val_loss: 1157864064.0000\n",
      "Epoch 675/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 274213664.0000 - val_loss: 1208069376.0000\n",
      "Epoch 676/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 252666976.0000 - val_loss: 1238726912.0000\n",
      "Epoch 677/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 250193632.0000 - val_loss: 1172006400.0000\n",
      "Epoch 678/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 262395840.0000 - val_loss: 1320529408.0000\n",
      "Epoch 679/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 264869824.0000 - val_loss: 1192426752.0000\n",
      "Epoch 680/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 260544032.0000 - val_loss: 1309164928.0000\n",
      "Epoch 681/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 252734752.0000 - val_loss: 1114085504.0000\n",
      "Epoch 682/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 263420992.0000 - val_loss: 1221155712.0000\n",
      "Epoch 683/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 281522624.0000 - val_loss: 1120116864.0000\n",
      "Epoch 684/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 260334208.0000 - val_loss: 1193823488.0000\n",
      "Epoch 685/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 262610656.0000 - val_loss: 1249857920.0000\n",
      "Epoch 686/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 267534400.0000 - val_loss: 1304074368.0000\n",
      "Epoch 687/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 278387840.0000 - val_loss: 1191105664.0000\n",
      "Epoch 688/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 319836032.0000 - val_loss: 2836575488.0000\n",
      "Epoch 689/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 472298304.0000 - val_loss: 1099025536.0000\n",
      "Epoch 690/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 273832224.0000 - val_loss: 1099402368.0000\n",
      "Epoch 691/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 276344000.0000 - val_loss: 1145796480.0000\n",
      "Epoch 692/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 279440512.0000 - val_loss: 1167571584.0000\n",
      "Epoch 693/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 254718912.0000 - val_loss: 1274437376.0000\n",
      "Epoch 694/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 256275776.0000 - val_loss: 1202963840.0000\n",
      "Epoch 695/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 252665792.0000 - val_loss: 1289853824.0000\n",
      "Epoch 696/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 266972848.0000 - val_loss: 1225531264.0000\n",
      "Epoch 697/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 249348736.0000 - val_loss: 1241979776.0000\n",
      "Epoch 698/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 261196624.0000 - val_loss: 1205401344.0000\n",
      "Epoch 699/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 281008320.0000 - val_loss: 1230501376.0000\n",
      "Epoch 700/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 261264112.0000 - val_loss: 1220078080.0000\n",
      "Epoch 701/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 244474896.0000 - val_loss: 1166982144.0000\n",
      "Epoch 702/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 266233712.0000 - val_loss: 1214539008.0000\n",
      "Epoch 703/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 260039584.0000 - val_loss: 1254039936.0000\n",
      "Epoch 704/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 261673360.0000 - val_loss: 1175277952.0000\n",
      "Epoch 705/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 261059152.0000 - val_loss: 1208348416.0000\n",
      "Epoch 706/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 271546144.0000 - val_loss: 1213448448.0000\n",
      "Epoch 707/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 239202496.0000 - val_loss: 1200574464.0000\n",
      "Epoch 708/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 258562416.0000 - val_loss: 1633618816.0000\n",
      "Epoch 709/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 267624368.0000 - val_loss: 1148402304.0000\n",
      "Epoch 710/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 249787152.0000 - val_loss: 1237909376.0000\n",
      "Epoch 711/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 278674816.0000 - val_loss: 1128060800.0000\n",
      "Epoch 712/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 284502912.0000 - val_loss: 1225227520.0000\n",
      "Epoch 713/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 259706256.0000 - val_loss: 1375908736.0000\n",
      "Epoch 714/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 255059744.0000 - val_loss: 1282393600.0000\n",
      "Epoch 715/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 256816448.0000 - val_loss: 1197456512.0000\n",
      "Epoch 716/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 255996304.0000 - val_loss: 1214512512.0000\n",
      "Epoch 717/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 257749296.0000 - val_loss: 1204660480.0000\n",
      "Epoch 718/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 321923104.0000 - val_loss: 1340622208.0000\n",
      "Epoch 719/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 255652592.0000 - val_loss: 1249711488.0000\n",
      "Epoch 720/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 269004736.0000 - val_loss: 1302471552.0000\n",
      "Epoch 721/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 233095456.0000 - val_loss: 1447148160.0000\n",
      "Epoch 722/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 239200288.0000 - val_loss: 1194856192.0000\n",
      "Epoch 723/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 235361968.0000 - val_loss: 1149772800.0000\n",
      "Epoch 724/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 251922064.0000 - val_loss: 1278385792.0000\n",
      "Epoch 725/1000\n",
      "117/117 [==============================] - 1s 9ms/step - loss: 253798224.0000 - val_loss: 1116282368.0000\n",
      "Epoch 726/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 232411408.0000 - val_loss: 1226087936.0000\n",
      "Epoch 727/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 237551648.0000 - val_loss: 1289757696.0000\n",
      "Epoch 728/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 308476704.0000 - val_loss: 1303301504.0000\n",
      "Epoch 729/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 241510032.0000 - val_loss: 1324038912.0000\n",
      "Epoch 730/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 240844688.0000 - val_loss: 1142488320.0000\n",
      "Epoch 731/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 244029472.0000 - val_loss: 1485971584.0000\n",
      "Epoch 732/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 323485824.0000 - val_loss: 1266938496.0000\n",
      "Epoch 733/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 233714208.0000 - val_loss: 1193279872.0000\n",
      "Epoch 734/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 255064768.0000 - val_loss: 1198736768.0000\n",
      "Epoch 735/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 240133968.0000 - val_loss: 1219973120.0000\n",
      "Epoch 736/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 242673376.0000 - val_loss: 1194953088.0000\n",
      "Epoch 737/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 258153232.0000 - val_loss: 1145554304.0000\n",
      "Epoch 738/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 249502032.0000 - val_loss: 1201712256.0000\n",
      "Epoch 739/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 228063968.0000 - val_loss: 1162628224.0000\n",
      "Epoch 740/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 257034384.0000 - val_loss: 1206160896.0000\n",
      "Epoch 741/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 247306992.0000 - val_loss: 1330902016.0000\n",
      "Epoch 742/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 288793472.0000 - val_loss: 1538838016.0000\n",
      "Epoch 743/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 266841264.0000 - val_loss: 1160272128.0000\n",
      "Epoch 744/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 246398912.0000 - val_loss: 1188554624.0000\n",
      "Epoch 745/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227386032.0000 - val_loss: 1435574144.0000\n",
      "Epoch 746/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 252251184.0000 - val_loss: 1019277120.0000\n",
      "Epoch 747/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 247235376.0000 - val_loss: 1185193856.0000\n",
      "Epoch 748/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 241786240.0000 - val_loss: 1222220032.0000\n",
      "Epoch 749/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 248765200.0000 - val_loss: 1178750080.0000\n",
      "Epoch 750/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 230781520.0000 - val_loss: 1473498240.0000\n",
      "Epoch 751/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 237287616.0000 - val_loss: 1254932096.0000\n",
      "Epoch 752/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 274673792.0000 - val_loss: 1215760768.0000\n",
      "Epoch 753/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 247331232.0000 - val_loss: 1221504000.0000\n",
      "Epoch 754/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 232898848.0000 - val_loss: 1153540480.0000\n",
      "Epoch 755/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 469772576.0000 - val_loss: 1138381568.0000\n",
      "Epoch 756/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 247394224.0000 - val_loss: 1199058432.0000\n",
      "Epoch 757/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 245333280.0000 - val_loss: 1233015040.0000\n",
      "Epoch 758/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 224777696.0000 - val_loss: 1246564224.0000\n",
      "Epoch 759/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 241383488.0000 - val_loss: 1102506624.0000\n",
      "Epoch 760/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 266918416.0000 - val_loss: 1225519232.0000\n",
      "Epoch 761/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 246770528.0000 - val_loss: 1163713664.0000\n",
      "Epoch 762/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228580752.0000 - val_loss: 1334980992.0000\n",
      "Epoch 763/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 249331296.0000 - val_loss: 1251326336.0000\n",
      "Epoch 764/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 223944192.0000 - val_loss: 1509695488.0000\n",
      "Epoch 765/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 282970624.0000 - val_loss: 1202592768.0000\n",
      "Epoch 766/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 302506560.0000 - val_loss: 1171965824.0000\n",
      "Epoch 767/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 266906912.0000 - val_loss: 1154409856.0000\n",
      "Epoch 768/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 248511776.0000 - val_loss: 1123335040.0000\n",
      "Epoch 769/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 218034000.0000 - val_loss: 1417198080.0000\n",
      "Epoch 770/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 247569280.0000 - val_loss: 1241244672.0000\n",
      "Epoch 771/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 262817792.0000 - val_loss: 1244803968.0000\n",
      "Epoch 772/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 273691200.0000 - val_loss: 1113140992.0000\n",
      "Epoch 773/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228720416.0000 - val_loss: 1238710912.0000\n",
      "Epoch 774/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 239743488.0000 - val_loss: 1214252672.0000\n",
      "Epoch 775/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 239155008.0000 - val_loss: 1322310912.0000\n",
      "Epoch 776/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 293448032.0000 - val_loss: 1447290752.0000\n",
      "Epoch 777/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 261589408.0000 - val_loss: 1173198208.0000\n",
      "Epoch 778/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 261410416.0000 - val_loss: 1169857792.0000\n",
      "Epoch 779/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 242422160.0000 - val_loss: 1323985152.0000\n",
      "Epoch 780/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 230338944.0000 - val_loss: 1220772096.0000\n",
      "Epoch 781/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 235162208.0000 - val_loss: 1240746368.0000\n",
      "Epoch 782/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 214141920.0000 - val_loss: 1216648320.0000\n",
      "Epoch 783/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 222315504.0000 - val_loss: 1231898752.0000\n",
      "Epoch 784/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 225668304.0000 - val_loss: 1215579648.0000\n",
      "Epoch 785/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 253269008.0000 - val_loss: 1307703040.0000\n",
      "Epoch 786/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 258107792.0000 - val_loss: 1128339968.0000\n",
      "Epoch 787/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 245751248.0000 - val_loss: 1219651968.0000\n",
      "Epoch 788/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 227816912.0000 - val_loss: 1597204608.0000\n",
      "Epoch 789/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 244489760.0000 - val_loss: 1223787008.0000\n",
      "Epoch 790/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 217367312.0000 - val_loss: 1222668672.0000\n",
      "Epoch 791/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 236561456.0000 - val_loss: 1265156992.0000\n",
      "Epoch 792/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 239097040.0000 - val_loss: 1421953408.0000\n",
      "Epoch 793/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 244129056.0000 - val_loss: 1284674560.0000\n",
      "Epoch 794/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 252743040.0000 - val_loss: 1280009472.0000\n",
      "Epoch 795/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 265962320.0000 - val_loss: 1434991872.0000\n",
      "Epoch 796/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 238528096.0000 - val_loss: 1184646016.0000\n",
      "Epoch 797/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 223293504.0000 - val_loss: 1232856320.0000\n",
      "Epoch 798/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 227575744.0000 - val_loss: 1261523456.0000\n",
      "Epoch 799/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 245559488.0000 - val_loss: 1388878336.0000\n",
      "Epoch 800/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 261041104.0000 - val_loss: 1249236608.0000\n",
      "Epoch 801/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 240424960.0000 - val_loss: 1262089984.0000\n",
      "Epoch 802/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 285728672.0000 - val_loss: 1762645760.0000\n",
      "Epoch 803/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 248141408.0000 - val_loss: 1194582528.0000\n",
      "Epoch 804/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 216912176.0000 - val_loss: 1242785280.0000\n",
      "Epoch 805/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 237999872.0000 - val_loss: 1308370304.0000\n",
      "Epoch 806/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 241111936.0000 - val_loss: 1362470784.0000\n",
      "Epoch 807/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 237272384.0000 - val_loss: 1281702528.0000\n",
      "Epoch 808/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 267310688.0000 - val_loss: 1230806656.0000\n",
      "Epoch 809/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 266225152.0000 - val_loss: 1349207168.0000\n",
      "Epoch 810/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 225330352.0000 - val_loss: 1297644160.0000\n",
      "Epoch 811/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 218085840.0000 - val_loss: 1238716928.0000\n",
      "Epoch 812/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 210629392.0000 - val_loss: 1282306944.0000\n",
      "Epoch 813/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 235690544.0000 - val_loss: 1231132160.0000\n",
      "Epoch 814/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 227394336.0000 - val_loss: 1190886784.0000\n",
      "Epoch 815/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227818992.0000 - val_loss: 1210512256.0000\n",
      "Epoch 816/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228427568.0000 - val_loss: 1224832768.0000\n",
      "Epoch 817/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227891024.0000 - val_loss: 1244998272.0000\n",
      "Epoch 818/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 260181600.0000 - val_loss: 1296543232.0000\n",
      "Epoch 819/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 322541696.0000 - val_loss: 1203776896.0000\n",
      "Epoch 820/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 257032832.0000 - val_loss: 1290741120.0000\n",
      "Epoch 821/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 241707072.0000 - val_loss: 1394391936.0000\n",
      "Epoch 822/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 246734736.0000 - val_loss: 1295816832.0000\n",
      "Epoch 823/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 222722672.0000 - val_loss: 1208023424.0000\n",
      "Epoch 824/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 210298336.0000 - val_loss: 1239260160.0000\n",
      "Epoch 825/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 275264736.0000 - val_loss: 1310766080.0000\n",
      "Epoch 826/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 219267456.0000 - val_loss: 1267511040.0000\n",
      "Epoch 827/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 219356080.0000 - val_loss: 1222652416.0000\n",
      "Epoch 828/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 221386080.0000 - val_loss: 1279821312.0000\n",
      "Epoch 829/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 239933520.0000 - val_loss: 1514503424.0000\n",
      "Epoch 830/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 225824784.0000 - val_loss: 1565041408.0000\n",
      "Epoch 831/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 224390000.0000 - val_loss: 1470128768.0000\n",
      "Epoch 832/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 207689568.0000 - val_loss: 1390481920.0000\n",
      "Epoch 833/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 256616736.0000 - val_loss: 1155278464.0000\n",
      "Epoch 834/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 235101376.0000 - val_loss: 1220289408.0000\n",
      "Epoch 835/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 238384448.0000 - val_loss: 1715323776.0000\n",
      "Epoch 836/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 240323968.0000 - val_loss: 1286743424.0000\n",
      "Epoch 837/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 223407088.0000 - val_loss: 1285312512.0000\n",
      "Epoch 838/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 258891856.0000 - val_loss: 1246269952.0000\n",
      "Epoch 839/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 248761664.0000 - val_loss: 1276191744.0000\n",
      "Epoch 840/1000\n",
      "117/117 [==============================] - 1s 8ms/step - loss: 222786048.0000 - val_loss: 1315832320.0000\n",
      "Epoch 841/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 214680848.0000 - val_loss: 1280561920.0000\n",
      "Epoch 842/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 215114128.0000 - val_loss: 1238406656.0000\n",
      "Epoch 843/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 243907904.0000 - val_loss: 1415515264.0000\n",
      "Epoch 844/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 225582064.0000 - val_loss: 1205407232.0000\n",
      "Epoch 845/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 204462368.0000 - val_loss: 1302350336.0000\n",
      "Epoch 846/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 219745184.0000 - val_loss: 1393780736.0000\n",
      "Epoch 847/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 217024688.0000 - val_loss: 1417965312.0000\n",
      "Epoch 848/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 248708384.0000 - val_loss: 1158788608.0000\n",
      "Epoch 849/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 217285808.0000 - val_loss: 1324557696.0000\n",
      "Epoch 850/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 253413632.0000 - val_loss: 1359397120.0000\n",
      "Epoch 851/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 325787808.0000 - val_loss: 1411393792.0000\n",
      "Epoch 852/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 233575808.0000 - val_loss: 1371178112.0000\n",
      "Epoch 853/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 236764064.0000 - val_loss: 1290468608.0000\n",
      "Epoch 854/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 210423296.0000 - val_loss: 1233753600.0000\n",
      "Epoch 855/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 222478384.0000 - val_loss: 1311996160.0000\n",
      "Epoch 856/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 215073872.0000 - val_loss: 1358593664.0000\n",
      "Epoch 857/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 219437120.0000 - val_loss: 1390524288.0000\n",
      "Epoch 858/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 229331792.0000 - val_loss: 1221089536.0000\n",
      "Epoch 859/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 224987536.0000 - val_loss: 1336863360.0000\n",
      "Epoch 860/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 213759376.0000 - val_loss: 1386181248.0000\n",
      "Epoch 861/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 202512384.0000 - val_loss: 1371171072.0000\n",
      "Epoch 862/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 201325504.0000 - val_loss: 1282102400.0000\n",
      "Epoch 863/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 208960112.0000 - val_loss: 1275126656.0000\n",
      "Epoch 864/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 248745568.0000 - val_loss: 1477927168.0000\n",
      "Epoch 865/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 266750400.0000 - val_loss: 1225342720.0000\n",
      "Epoch 866/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 210964272.0000 - val_loss: 1391023616.0000\n",
      "Epoch 867/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 231888000.0000 - val_loss: 1323567104.0000\n",
      "Epoch 868/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 213868112.0000 - val_loss: 1309645312.0000\n",
      "Epoch 869/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 207345760.0000 - val_loss: 1389586816.0000\n",
      "Epoch 870/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 199574720.0000 - val_loss: 1410277888.0000\n",
      "Epoch 871/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 201560288.0000 - val_loss: 1406872960.0000\n",
      "Epoch 872/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 211427152.0000 - val_loss: 1309293184.0000\n",
      "Epoch 873/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 226086128.0000 - val_loss: 1286363008.0000\n",
      "Epoch 874/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 272483776.0000 - val_loss: 1434291968.0000\n",
      "Epoch 875/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 233919408.0000 - val_loss: 1386984576.0000\n",
      "Epoch 876/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 231162480.0000 - val_loss: 1438574208.0000\n",
      "Epoch 877/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 251943104.0000 - val_loss: 1539087232.0000\n",
      "Epoch 878/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 209224528.0000 - val_loss: 1473279104.0000\n",
      "Epoch 879/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 270746208.0000 - val_loss: 1343960064.0000\n",
      "Epoch 880/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 211635136.0000 - val_loss: 1430200576.0000\n",
      "Epoch 881/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 235938384.0000 - val_loss: 1629048832.0000\n",
      "Epoch 882/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 226550688.0000 - val_loss: 1396679296.0000\n",
      "Epoch 883/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228111888.0000 - val_loss: 1412960256.0000\n",
      "Epoch 884/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227283504.0000 - val_loss: 1293485568.0000\n",
      "Epoch 885/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 222136848.0000 - val_loss: 1383583104.0000\n",
      "Epoch 886/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 201855440.0000 - val_loss: 1337568384.0000\n",
      "Epoch 887/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 235273728.0000 - val_loss: 1296799488.0000\n",
      "Epoch 888/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 224344400.0000 - val_loss: 1376448512.0000\n",
      "Epoch 889/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227800912.0000 - val_loss: 1389063552.0000\n",
      "Epoch 890/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 219352176.0000 - val_loss: 1328938240.0000\n",
      "Epoch 891/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 210873072.0000 - val_loss: 1323611904.0000\n",
      "Epoch 892/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 208806592.0000 - val_loss: 1639916928.0000\n",
      "Epoch 893/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 214646688.0000 - val_loss: 1365882624.0000\n",
      "Epoch 894/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 216833472.0000 - val_loss: 1387041792.0000\n",
      "Epoch 895/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 212254512.0000 - val_loss: 1369797120.0000\n",
      "Epoch 896/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 236254304.0000 - val_loss: 1322163968.0000\n",
      "Epoch 897/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 208454304.0000 - val_loss: 1339829376.0000\n",
      "Epoch 898/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 219291904.0000 - val_loss: 1390582528.0000\n",
      "Epoch 899/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 203839648.0000 - val_loss: 1351205376.0000\n",
      "Epoch 900/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 209546304.0000 - val_loss: 1546839808.0000\n",
      "Epoch 901/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 241321024.0000 - val_loss: 1310140032.0000\n",
      "Epoch 902/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 202610032.0000 - val_loss: 1347879680.0000\n",
      "Epoch 903/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 229749248.0000 - val_loss: 1336525440.0000\n",
      "Epoch 904/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 211088096.0000 - val_loss: 1683090432.0000\n",
      "Epoch 905/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 200565056.0000 - val_loss: 1460438912.0000\n",
      "Epoch 906/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 207291920.0000 - val_loss: 1585349248.0000\n",
      "Epoch 907/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228880544.0000 - val_loss: 1390993408.0000\n",
      "Epoch 908/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 377474496.0000 - val_loss: 1617166720.0000\n",
      "Epoch 909/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 207373008.0000 - val_loss: 1368134528.0000\n",
      "Epoch 910/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 203589408.0000 - val_loss: 1420424064.0000\n",
      "Epoch 911/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 204390800.0000 - val_loss: 1303578240.0000\n",
      "Epoch 912/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 204505168.0000 - val_loss: 1339338880.0000\n",
      "Epoch 913/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 202406352.0000 - val_loss: 1499111168.0000\n",
      "Epoch 914/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 248457856.0000 - val_loss: 1253421440.0000\n",
      "Epoch 915/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 417555008.0000 - val_loss: 1646715904.0000\n",
      "Epoch 916/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 257328672.0000 - val_loss: 1554942976.0000\n",
      "Epoch 917/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 198631696.0000 - val_loss: 1475822976.0000\n",
      "Epoch 918/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 212971520.0000 - val_loss: 1391214208.0000\n",
      "Epoch 919/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 205907472.0000 - val_loss: 1541447296.0000\n",
      "Epoch 920/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 195208768.0000 - val_loss: 1410832128.0000\n",
      "Epoch 921/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 202071648.0000 - val_loss: 1389077888.0000\n",
      "Epoch 922/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 217735120.0000 - val_loss: 1477017344.0000\n",
      "Epoch 923/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 236937104.0000 - val_loss: 1367737728.0000\n",
      "Epoch 924/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 195699328.0000 - val_loss: 1496212480.0000\n",
      "Epoch 925/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 202599728.0000 - val_loss: 1404252416.0000\n",
      "Epoch 926/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 243451984.0000 - val_loss: 1538859520.0000\n",
      "Epoch 927/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 220647744.0000 - val_loss: 1481641728.0000\n",
      "Epoch 928/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 230962240.0000 - val_loss: 1434697856.0000\n",
      "Epoch 929/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 201202880.0000 - val_loss: 1780689920.0000\n",
      "Epoch 930/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 292456832.0000 - val_loss: 1307986176.0000\n",
      "Epoch 931/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 256269040.0000 - val_loss: 2003646080.0000\n",
      "Epoch 932/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 200403520.0000 - val_loss: 1462121728.0000\n",
      "Epoch 933/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 192528784.0000 - val_loss: 1523967744.0000\n",
      "Epoch 934/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 208445824.0000 - val_loss: 1509687936.0000\n",
      "Epoch 935/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 194377488.0000 - val_loss: 1380720768.0000\n",
      "Epoch 936/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 203206224.0000 - val_loss: 1410032128.0000\n",
      "Epoch 937/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 201417600.0000 - val_loss: 1517724288.0000\n",
      "Epoch 938/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 210277632.0000 - val_loss: 1663526784.0000\n",
      "Epoch 939/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 226195200.0000 - val_loss: 1569741696.0000\n",
      "Epoch 940/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 186553632.0000 - val_loss: 1437311616.0000\n",
      "Epoch 941/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 210791312.0000 - val_loss: 1474605184.0000\n",
      "Epoch 942/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 242731200.0000 - val_loss: 1450414976.0000\n",
      "Epoch 943/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 200016528.0000 - val_loss: 1462778624.0000\n",
      "Epoch 944/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 202067712.0000 - val_loss: 1418748928.0000\n",
      "Epoch 945/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 214316320.0000 - val_loss: 1318828160.0000\n",
      "Epoch 946/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 242159216.0000 - val_loss: 1547115776.0000\n",
      "Epoch 947/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 238820928.0000 - val_loss: 1432014336.0000\n",
      "Epoch 948/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 194991584.0000 - val_loss: 1456277760.0000\n",
      "Epoch 949/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 221044992.0000 - val_loss: 1508443136.0000\n",
      "Epoch 950/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 220761488.0000 - val_loss: 1405830016.0000\n",
      "Epoch 951/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 199097840.0000 - val_loss: 1731330816.0000\n",
      "Epoch 952/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 197055680.0000 - val_loss: 1396299904.0000\n",
      "Epoch 953/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 209468640.0000 - val_loss: 1562026624.0000\n",
      "Epoch 954/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 196069840.0000 - val_loss: 1376937984.0000\n",
      "Epoch 955/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 204102384.0000 - val_loss: 1329715200.0000\n",
      "Epoch 956/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 194943344.0000 - val_loss: 1391318272.0000\n",
      "Epoch 957/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 192134768.0000 - val_loss: 1511484544.0000\n",
      "Epoch 958/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 207489552.0000 - val_loss: 1494976640.0000\n",
      "Epoch 959/1000\n",
      "117/117 [==============================] - 1s 9ms/step - loss: 221192032.0000 - val_loss: 1725457280.0000\n",
      "Epoch 960/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 236158496.0000 - val_loss: 1467579136.0000\n",
      "Epoch 961/1000\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 215210144.0000 - val_loss: 1446957056.0000\n",
      "Epoch 962/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 193465328.0000 - val_loss: 1486435712.0000\n",
      "Epoch 963/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 185612352.0000 - val_loss: 1446433536.0000\n",
      "Epoch 964/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 196196464.0000 - val_loss: 1430412800.0000\n",
      "Epoch 965/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 201212480.0000 - val_loss: 1352584704.0000\n",
      "Epoch 966/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 222755712.0000 - val_loss: 1677805952.0000\n",
      "Epoch 967/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 200975248.0000 - val_loss: 1543072640.0000\n",
      "Epoch 968/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 193152624.0000 - val_loss: 1396166656.0000\n",
      "Epoch 969/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 203268096.0000 - val_loss: 1662304000.0000\n",
      "Epoch 970/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 196917296.0000 - val_loss: 1384714112.0000\n",
      "Epoch 971/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 211085712.0000 - val_loss: 1420942336.0000\n",
      "Epoch 972/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 177691616.0000 - val_loss: 1644024960.0000\n",
      "Epoch 973/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 178945216.0000 - val_loss: 1610170752.0000\n",
      "Epoch 974/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 227741072.0000 - val_loss: 1478779520.0000\n",
      "Epoch 975/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 206507728.0000 - val_loss: 1485550336.0000\n",
      "Epoch 976/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 224703200.0000 - val_loss: 1471316224.0000\n",
      "Epoch 977/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 203344960.0000 - val_loss: 1474122880.0000\n",
      "Epoch 978/1000\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 210535632.0000 - val_loss: 1370008704.0000\n",
      "Epoch 979/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 212328624.0000 - val_loss: 1741970432.0000\n",
      "Epoch 980/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 190080608.0000 - val_loss: 1610302336.0000\n",
      "Epoch 981/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 244511392.0000 - val_loss: 1688178432.0000\n",
      "Epoch 982/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 218983216.0000 - val_loss: 1424079488.0000\n",
      "Epoch 983/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 255665808.0000 - val_loss: 1709258112.0000\n",
      "Epoch 984/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 250563568.0000 - val_loss: 1587256064.0000\n",
      "Epoch 985/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 181056496.0000 - val_loss: 1388871680.0000\n",
      "Epoch 986/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 261278176.0000 - val_loss: 1359982208.0000\n",
      "Epoch 987/1000\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 203215120.0000 - val_loss: 1516818816.0000\n",
      "Epoch 988/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 185755280.0000 - val_loss: 1464974080.0000\n",
      "Epoch 989/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 199280192.0000 - val_loss: 1669888768.0000\n",
      "Epoch 990/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 187890320.0000 - val_loss: 1680716032.0000\n",
      "Epoch 991/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 231363856.0000 - val_loss: 1616920064.0000\n",
      "Epoch 992/1000\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 187888544.0000 - val_loss: 1528648320.0000\n",
      "Epoch 993/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 228241952.0000 - val_loss: 1482030080.0000\n",
      "Epoch 994/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 209997456.0000 - val_loss: 1427441920.0000\n",
      "Epoch 995/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 197183008.0000 - val_loss: 1641457024.0000\n",
      "Epoch 996/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 206273360.0000 - val_loss: 1453813248.0000\n",
      "Epoch 997/1000\n",
      "117/117 [==============================] - 1s 7ms/step - loss: 180456608.0000 - val_loss: 1437699968.0000\n",
      "Epoch 998/1000\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 201787248.0000 - val_loss: 1478812032.0000\n",
      "Epoch 999/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 212650576.0000 - val_loss: 1448556672.0000\n",
      "Epoch 1000/1000\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 182260752.0000 - val_loss: 1893075840.0000\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(50, kernel_initializer= 'he_uniform', activation= 'relu', input_dim= 176))\n",
    "classifier.add(Dense(25, kernel_initializer= 'he_uniform', activation= 'relu'))\n",
    "classifier.add(Dense(50, kernel_initializer= 'he_uniform', activation= 'relu'))\n",
    "classifier.add(Dense(1, kernel_initializer= 'he_uniform'))\n",
    "\n",
    "classifier.compile(loss='mse', optimizer= 'adam')\n",
    "\n",
    "model_history = classifier.fit(X_train, y_train, validation_split=0.20, batch_size=10, epochs=1000) # epoch=1000 , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "ann_pred = classifier.predict(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
